---
title: PagedAttention
date: 2025-05-23
readingTime: 600
category:
  - 笔记
tag:
  - GPU优化
  - 大模型
  - 算法优化
# cover: /assets/images/cover3.jpg
isOriginal: true
---

# Flashattention

## 简介

PagedAttention是vLLM中提出的一个用于加速Transformer模型推理的注意力机制计算方法。

<!-- more -->