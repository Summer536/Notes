# GEMM

HPCä¼˜åŒ–çš„æ ¸å¿ƒæ€æƒ³ï¼Œæ€ä¹ˆæ ·è®©æ•°æ®æ”¾åœ¨æ›´è¿‘çš„å­˜å‚¨ä¸Šæ¥æ©ç›–è®¡ç®—çš„å»¶æ—¶ï¼Œä»è€Œå‡å°‘å­˜å‚¨å¢™çš„å½±å“ã€‚<br>
è®¡ç®—$C = A * B$ï¼Œå…¶ä¸­Aæ˜¯M * Kçš„çŸ©é˜µï¼ŒBæ˜¯K * Nçš„çŸ©é˜µï¼ŒCæ˜¯M * Nçš„çŸ©é˜µï¼Œæ•°æ®ç±»å‹æ˜¯Float32ã€‚

## V1. navie gemm
### å‚æ•°è®¾ç½®
const int M = 2048;<br>
const int N = 2048;<br>
const int K = 2048;
![](Figure/gemm/navie.jpg)

```cpp
__global__ void matrixMulGPU(float* A, float* B, float* C, int M, int N, int K) {
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;
    
    if (row < M && col < N) {
        for (int k = 0; k < K; ++k) {
            sum += A[row * K + k] * B[k * N + col];
        }
        C[row * N + col] = sum;
    }
}
```
- æ¯ä¸ªthreadè´Ÿè´£è¯»å–AçŸ©é˜µçš„ä¸€è¡Œå’ŒBçŸ©é˜µçš„ä¸€åˆ—ï¼Œè®¡ç®—å¾—åˆ°CçŸ©é˜µçš„ä¸€ä¸ªå…ƒç´ ï¼Œä¸€å…±éœ€è¦M*Nä¸ªthreadï¼›

### æ€§èƒ½åˆ†æ
- åˆ†æå¯çŸ¥ï¼Œçº¿ç¨‹æ¯æ¬¡è®¡ç®—ä¸€ä¸ªcçŸ©é˜µçš„å…ƒç´ ï¼Œéƒ½éœ€è¦ä»AçŸ©é˜µå’ŒBçŸ©é˜µä¸­åˆ†åˆ«è¯»å–Kä¸ªå…ƒç´ ï¼Œæ‰€ä»¥ä»global memoryä¸­è¯»å–çš„æ¬¡æ•°ä¸º2K*MNï¼Œå†™çš„æ¬¡æ•°ä¸ºM*Nã€‚ç”±äºåœ¨è¯»å–æ•°æ®æ¶ˆè€—äº†å¤§é‡çš„æ—¶é—´ï¼Œæ— æ³•å……åˆ†å‘æŒ¥GPUçš„ç®—åŠ›ã€‚
## V2. shanred memory 
> ğŸ’¡ **Note**å¦‚ä½•ç¡®å®šåˆ†å—æ•°é‡ï¼Ÿæ¯ä¸€ä¸ª Block è¯¥è´Ÿè´£å¤šå¤§çš„çŸ©é˜µä¹˜ï¼Ÿæ¯ä¸€ä¸ª thread åˆåº”è¯¥è´Ÿè´£å¤šå¤§çš„çŸ©é˜µä¹˜ï¼Ÿ[https://zhuanlan.zhihu.com/p/688610975]<br>

> ğŸ’¡ **Note**ä¸ºä»€ä¹ˆæ²¿ç€kç»´åº¦åˆ‡?<br>
![](Figure/gemm/qie.jpg)
å¦‚æœæŒ‰ç…§è¿™ç§æ–¹æ³•åˆ‡å—çš„è¯ï¼Œä¼šé‡å¤è¯»å–æ•°æ®ã€‚ä¾‹å¦‚å¯¹äºå›¾ä¸­çš„ä¸€å—Aï¼ˆé«˜äº®ï¼‰ï¼Œå®ƒå’ŒBä¸­çš„è‹¥å¹²å—å¯¹åº”ï¼Œä¹Ÿå°±æ„å‘³ç€Açš„è¿™ä¸ªåˆ†å—ä¼šè¢«é‡å¤åŠ è½½è‹¥å¹²æ¬¡ï¼ˆå’Œnaive GEMMæ˜¯ä¸€ä¸ªé“ç†ï¼‰ã€‚ä½†æ˜¯å¦‚æœæˆ‘ä»¬ç«–ç€åˆ‡Aï¼Œæ¨ªç€åˆ‡Bï¼ˆæ­¤æ—¶Aå’ŒBéƒ½æ˜¯æ²¿ç€Kæ–¹å‘åˆ‡å‰²çš„ï¼‰ï¼Œè¿™æ ·æ‰€æœ‰çš„Aåˆ†å—å’ŒBåˆ†å—éƒ½åªä¼šè¢«åŠ è½½1æ¬¡ã€‚å¯ä»¥èƒ½å¸®åŠ©æˆ‘ä»¬èŠ‚çœåŠ è½½æ•°æ®çš„æ—¶é—´ã€‚
![](Figure/gemm/g-s.jpg)
```cpp
static_assert((bm * bk + bk * bn) * sizeof(float) <= 48 * 1024, "share memory is out of 48KB!");

__global__ void matrixMulGPU(float* A, float* B, float* C, int M, int N, int K) { 
    int tidx = threadIdx.x;
    int tidy = threadIdx.y;
    int row = blockIdx.y * blockDim.y + threadIdx.y;
    int col = blockIdx.x * blockDim.x + threadIdx.x;
    float sum = 0.0f;

    //define shared memory
    __shared__ float sA[bm][bk];
    __shared__ float sB[bk][bn];

    //rewrite
    //æ¯ä¸ªçº¿ç¨‹ç»´æŠ¤è‡ªå·±çš„ sum å¯„å­˜å™¨ï¼Œæ¯è½® tile è®¡ç®—è´¡çŒ®åŠ åˆ° sum ä¸­ï¼Œæœ€åä¸€æ¬¡æ€§æŠŠæ€»å’Œå†™å›å…¨å±€å†…å­˜ã€‚
    for (int bk_id = 0; bk_id < K; bk_id += bk){        //for (int bk_id = 0, bk_id < K, bk_id += bk) // é”™è¯¯ï¼šé€—å·åº”ä¸ºåˆ†å·

        if (row < M && bk_id + tidx < K){
            sA[tidy][tidx] = A[(row * K) + (bk_id + tidx)];
        } else{
            sA[tidy][tidx] = 0.0f;
        }

        if (col < N && bk_id + tidy < K){
            sB[tidy][tidx] = B[((bk_id + tidy)) * N + (col)];
        } else{
            sB[tidy][tidx] = 0.0f;
        }

        __syncthreads();

        for (int k = 0; k < bk;  ++k){
            sum += sA[tidy][k] * sB[k][tidx];
        }
        __syncthreads(); 
    }

    if (row < M && col < N) {  
        C[row * N + col] = sum;
    }
}
```
![](Figure/gemm/shared%202.jpg)
- æ¯æ¬¡å–AçŸ©é˜µçš„ä¸€ä¸ªåˆ†å—ï¼ˆbm,bkï¼‰ï¼Œå–BçŸ©é˜µçš„ä¸€ä¸ªåˆ†å—ï¼ˆbk,bnï¼‰ï¼Œå°†ä¸¤è€…ç›¸ä¹˜å¾—åˆ°åˆ†å—çŸ©é˜µCï¼ˆbm,bnï¼‰;
- å¯¹AçŸ©é˜µï¼Œå‘å³æ‰¾åˆ°ä¸‹ä¸€ä¸ªåˆ†å—ï¼›å¯¹BçŸ©é˜µï¼Œå‘ä¸‹æ‰¾åˆ°ä¸‹ä¸€ä¸ªåˆ†å—ï¼Œç„¶åå†ç›¸ä¹˜å¾—åˆ°åˆ†å—çŸ©é˜µCï¼Œç´¯åŠ åˆ°ä¸Šä¸€ä¸ªåˆ†å—çŸ©é˜µCä¸Š;
- å¦‚æ­¤å¾ªç¯ï¼Œå½“æˆ‘ä»¬éå†å®Œæ‰€æœ‰çš„Aåˆ†å—å’ŒBåˆ†å—åï¼Œå°±å¯ä»¥å¾—åˆ°æœ€ç»ˆçš„åˆ†å—çŸ©é˜µCäº†ã€‚ä¹Ÿå°±æ˜¯æˆ‘ä»¬å›¾ä¸­çš„é«˜äº®ç»¿å—ï¼ˆbm,bnï¼‰ã€‚
### æ€§èƒ½åˆ†æ
- æ€§èƒ½è®¡ç®—
  - è®¡ç®—ä¸€ä¸ªå°ºå¯¸ä¸ºbm*bnçš„çŸ©é˜µCï¼Œéœ€è¦ä»global memoryä¸­è¯»å–ï¼ˆbm,bkï¼‰çš„Aå’Œï¼ˆbk,bnï¼‰çš„Bï¼Œä»global memoryä¸­è¯»å–çš„æ¬¡æ•°ä¸ºbm*bk+bk*bn,éœ€è¦è®¡ç®—$\frac{K}{bk}$æ¬¡ï¼Œæ‰€ä»¥æ€»çš„è¯»å–æ¬¡æ•°ä½Kï¼ˆbm+bnï¼‰;
   - æ€»çš„blockæ•°ç›®ä¸ºï¼š$\frac{M * N}{bm * bn}$;
   - æœ€ç»ˆçš„è¯»å–æ¬¡æ•°ä¸ºï¼š$MNK(\frac{1}{bm} + \frac{1}{bn})$<br>
ç”±æ­¤ä¸v1ç›¸æ¯”ï¼Œè®¿å­˜é‡å‡å°‘ä¸º$\frac{1}{2}*(\frac{1}{bn} + \frac{1}{bm})$ï¼Œå¯çŸ¥bn,bmè¶Šå¤§æ—¶ï¼Œåˆ†å—æƒ…å†µä¸‹å¯¹global memoryçš„è¯»å†™æ¬¡æ•°è¶Šå°‘ï¼Œä½¿å¾—gpuç›¸å¯¹èŠ±æ›´å¤šçš„æ—¶é—´åœ¨è®¡ç®—è€Œä¸æ˜¯åœ¨è¯»æ•°ä¸Šï¼Œæ›´æœ‰æ•ˆåˆ©ç”¨gpuã€‚
- å¾…ä¼˜åŒ–ç‚¹<br>
å¯ä»¥å°†global memoryåˆ†å—åŠ è½½åˆ°shared memoryä¸­ï¼Œé‚£æ˜¯å¦å¯ä»¥å°†shared memoryåˆ†å—åŠ è½½åˆ°registerä¸­ï¼Ÿ
## V3. ä¸€ä¸ªçŸ©é˜µè®¡ç®—å¤šä¸ªå…ƒç´ 
![](Figure/gemm/thread.jpg)
```cpp
static_assert((bm * bk + bk * bn) * sizeof(float) <= 48 * 1024, "share memory is out of 48KB!");

__global__ void matrixMulGPU(float* A, float* B, float* C, int M, int N, int K) { 
    //change the one thread calculate [1] to [1,2
    //                                        3,4]
    int tidx = threadIdx.x * size;
    int tidy = threadIdx.y * size;
    int row = blockIdx.y * size * blockDim.y + threadIdx.y * size;
    int col = blockIdx.x * size * blockDim.x + threadIdx.x * size;
    //change the float to arrays
    float sum[size][size] = {0.0f};

    __shared__ float sA[bm][bk];
    __shared__ float sB[bk][bn];

    for (int bk_id = 0; bk_id < K; bk_id += bk){   

        // Here is threadIdx.x instead of tidx
        if (row < M && bk_id + threadIdx.x < K){
            // We must put two rows data into sharemem
            sA[tidy + 0][threadIdx.x] = A[((row + 0) * K) + (bk_id + threadIdx.x)];
            sA[tidy + 1][threadIdx.x] = A[((row + 1) * K) + (bk_id + threadIdx.x)];
        }

        // Here is threadIdx.y instead of tidy
        if (bk_id + threadIdx.y < K && col < N){ //think this conditioon
            sB[threadIdx.y][tidx + 0] = B[((bk_id + threadIdx.y) * N) + (col + 0)];
            sB[threadIdx.y][tidx + 1] = B[((bk_id + threadIdx.y) * N) + (col + 1)];
        }

        __syncthreads();

        //Extract the (size*size) float data from the array separately
        for (int k = 0; k < bk;  ++k){
            float a0 = sA[tidy + 0][k];
            float a1 = sA[tidy + 1][k];
            float b0 = sB[k][tidx + 0];
            float b1 = sB[k][tidx + 1];

            sum[0][0] += a0 * b0;  //bug1: here is += instead of =
            sum[0][1] += a0 * b1;
            sum[1][0] += a1 * b0;
            sum[1][1] += a1 * b1;
        }
        __syncthreads();
    }

    if ((row + 0)  < M && (col+0) < N) C[(row + 0) * N + (col+0)] = sum[0][0];
    if ((row + 1)  < M && (col+0) < N) C[(row + 1) * N + (col+0)] = sum[1][0];
    if ((row + 0)  < M && (col+1) < N) C[(row + 0) * N + (col+1)] = sum[0][1];
    if ((row + 1)  < M && (col+1) < N) C[(row + 1) * N + (col+1)] = sum[1][1];
}
```
å¯¹äºä¸€ä¸ªçº¿ç¨‹è€Œè¨€ï¼Œå®ƒç°åœ¨æ‹¥æœ‰ï¼štm * tkä¸ªAçŸ©é˜µçš„å¯„å­˜å™¨å€¼ï¼Œtk * tnä¸ªBçŸ©é˜µçš„å¯„å­˜å™¨å€¼ï¼Œä»¥åŠtm * tnä¸ªCçŸ©é˜µçš„å¯„å­˜å™¨å€¼ã€‚é€šè¿‡è¿™äº›å¯„å­˜å™¨çš„å€¼ï¼Œéœ€è¦è®¡ç®—tm * tnä¸ªæ•°ã€‚è¿™éœ€è¦tm * tnæ¡FFMAæŒ‡ä»¤ã€‚
### æ€§èƒ½åˆ†æ
- ç”±ä¸Šé¢å¯¹global memoryçš„åˆ†æå¯çŸ¥ï¼Œå¯¹shared memoryè¿›è¡Œåˆ†å—è®¿å­˜é‡ä¼šå‡å°‘$\frac{1}{2}*(\frac{1}{tn} + \frac{1}{tm})$ã€‚<br>
- ä½¿ç”¨å‘é‡åŒ–è¯»å–
## V4. å‘é‡åŒ–è¯»å–
> ğŸ’¡ **Note** ä½¿ç”¨float4çš„ä¼˜ç‚¹ï¼Ÿ<br>

> â **warning** AçŸ©é˜µä¸èƒ½ç”¨float4è¯»å–ï¼Œå› ä¸ºæˆ‘ä»¬è¿™è¾¹è¦è¯»å–çš„AçŸ©é˜µæ˜¯å››ä¸ªåˆ—æ–¹å‘è¿ç»­çš„å…ƒç´ ã€‚å¦‚æœç›´æ¥ä½¿ç”¨å‘é‡åŒ–è¯»å–çš„è¯ï¼Œæ˜¯è¯»å–çš„è¡Œæ–¹å‘ï¼
```cpp
static_assert((bm * bk + bk * bn) * sizeof(float) <= 48 * 1024, "share memory is out of 48KB!");

__global__ void matrixMulGPU(float* A, float* B, float* C, int M, int N, int K) { 

    int tidx = threadIdx.x * size;
    int tidy = threadIdx.y * size;
    int row = blockIdx.y * size * blockDim.y + threadIdx.y * size;
    int col = blockIdx.x * size * blockDim.x + threadIdx.x * size;

    float sum[size][size] = {0.0f};

    __shared__ float sA[bm][bk];
    __shared__ float sB[bk][bn];

    // float4* A_float4 = reinterpret_cast<float4*>(A);
    float4* B_float4 = reinterpret_cast<float4*>(B);

    for (int bk_id = 0; bk_id < K; bk_id += bk) {   
          
        if (row < M && bk_id + threadIdx.x < K) {
            // AçŸ©é˜µæ”¹å›æ™®é€šè¯»å–
            sA[tidy + 0][threadIdx.x] = A[(row + 0) * K + bk_id + threadIdx.x];
            sA[tidy + 1][threadIdx.x] = A[(row + 1) * K + bk_id + threadIdx.x];
            sA[tidy + 2][threadIdx.x] = A[(row + 2) * K + bk_id + threadIdx.x];
            sA[tidy + 3][threadIdx.x] = A[(row + 3) * K + bk_id + threadIdx.x];
        }

        if (bk_id + threadIdx.y < K && col < N) {
            // å‘é‡åŒ–è¯»å–BçŸ©é˜µæ•°æ®
            float4 b_vec = B_float4[((bk_id + threadIdx.y) * N + col) / 4]; //å¯ä»¥æƒ³è±¡ä¸ºBçŸ©é˜µçš„åˆ—æ•°æ•´ä½“ç¼©å°ä¸º1/4
            sB[threadIdx.y][tidx + 0] = b_vec.x;
            sB[threadIdx.y][tidx + 1] = b_vec.y;
            sB[threadIdx.y][tidx + 2] = b_vec.z;
            sB[threadIdx.y][tidx + 3] = b_vec.w;
        }

        __syncthreads();

        #pragma unroll
        for (int k = 0; k < bk; ++k) {
            float a0 = sA[tidy + 0][k];
            float a1 = sA[tidy + 1][k];
            float a2 = sA[tidy + 2][k];
            float a3 = sA[tidy + 3][k];

            float4 b_reg = make_float4(sB[k][tidx + 0], //b0
                                      sB[k][tidx + 1],  //b1
                                      sB[k][tidx + 2],  //b2
                                      sB[k][tidx + 3]); //b3

            sum[0][0] += a0 * b_reg.x;
            sum[0][1] += a0 * b_reg.y;
            sum[0][2] += a0 * b_reg.z;
            sum[0][3] += a0 * b_reg.w;
            sum[1][0] += a1 * b_reg.x;
            sum[1][1] += a1 * b_reg.y;
            sum[1][2] += a1 * b_reg.z;
            sum[1][3] += a1 * b_reg.w;
            sum[2][0] += a2 * b_reg.x;
            sum[2][1] += a2 * b_reg.y;
            sum[2][2] += a2 * b_reg.z;
            sum[2][3] += a2 * b_reg.w;
            sum[3][0] += a3 * b_reg.x;
            sum[3][1] += a3 * b_reg.y;
            sum[3][2] += a3 * b_reg.z;
            sum[3][3] += a3 * b_reg.w;
        }
        __syncthreads();
    }

    if ((row + 0)  < M && (col+0) < N) C[(row + 0) * N + (col+0)] = sum[0][0];
    if ((row + 0)  < M && (col+1) < N) C[(row + 0) * N + (col+1)] = sum[0][1];
    if ((row + 0)  < M && (col+2) < N) C[(row + 0) * N + (col+2)] = sum[0][2];
    if ((row + 0)  < M && (col+3) < N) C[(row + 0) * N + (col+3)] = sum[0][3];
    if ((row + 1)  < M && (col+0) < N) C[(row + 1) * N + (col+0)] = sum[1][0];
    if ((row + 1)  < M && (col+1) < N) C[(row + 1) * N + (col+1)] = sum[1][1];
    if ((row + 1)  < M && (col+2) < N) C[(row + 1) * N + (col+2)] = sum[1][2];
    if ((row + 1)  < M && (col+3) < N) C[(row + 1) * N + (col+3)] = sum[1][3];
    if ((row + 2)  < M && (col+0) < N) C[(row + 2) * N + (col+0)] = sum[2][0];
    if ((row + 2)  < M && (col+1) < N) C[(row + 2) * N + (col+1)] = sum[2][1];
    if ((row + 2)  < M && (col+2) < N) C[(row + 2) * N + (col+2)] = sum[2][2];
    if ((row + 2)  < M && (col+3) < N) C[(row + 2) * N + (col+3)] = sum[2][3];
    if ((row + 3)  < M && (col+0) < N) C[(row + 3) * N + (col+0)] = sum[3][0];
    if ((row + 3)  < M && (col+1) < N) C[(row + 3) * N + (col+1)] = sum[3][1];
    if ((row + 3)  < M && (col+2) < N) C[(row + 3) * N + (col+2)] = sum[3][2];
    if ((row + 3)  < M && (col+3) < N) C[(row + 3) * N + (col+3)] = sum[3][3];
}
```
### æ€§èƒ½åˆ†æ
![](Figure/gemm/bank%20conflict.jpg)
- å‘é‡åŒ–è¯»å–æ•°æ®æ—¶ï¼Œwarpä¼šé‡‡ç”¨LDS.128æŒ‡ä»¤ï¼Œä¸€ä¸ªwarpå…±éœ€å–4*32 = 128ä¸ªæ•°ï¼Œå·²ç»è¶…è¿‡warpå•æ¬¡memory transactionå…è®¸çš„å–æ•°ä¸Šé™ï¼ˆé€šå¸¸æ¯æ¬¡æœ€å¤šè¯»å– 128 å­—èŠ‚ï¼‰ã€‚æ‰€ä»¥è¯¥warpä¼šæŠŠå–æ•°è¿‡ç¨‹æ‹†æˆ4ä¸ªä¸²è¡Œçš„phaseï¼ˆå³4æ¬¡ä¸²è¡Œçš„memory transcationï¼‰ï¼šå³0ï½7ï¼Œ8ï½15ï¼Œ16ï½23ï¼Œ24ï½31ã€‚è¿™æ—¶bank conflictè¢«å®šä¹‰åœ¨æ¯ä¸ªphaseï¼ˆä¹Ÿå°±æ˜¯1/4ä¸ªwarpä¹‹å†…ï¼‰ã€‚
## V5. BçŸ©é˜µå‘é‡åŒ–è¯»å– + åŒç¼“å†²ä¼˜åŒ–
```cpp
// ç¡®ä¿å…±äº«å†…å­˜ä¸è¶…è¿‡é™åˆ¶ (ç°åœ¨éœ€è¦ä¸¤å€çš„å…±äº«å†…å­˜ç©ºé—´ç”¨äºåŒç¼“å†²)
static_assert(2 * (bm * bk + bk * bn) * sizeof(float) <= 48 * 1024, "share memory is out of 48KB!");

__global__ void matrixMulGPU(float* A, float* B, float* C, int M, int N, int K) { 

    int tidx = threadIdx.x * size;
    int tidy = threadIdx.y * size;
    int row = blockIdx.y * size * blockDim.y + threadIdx.y * size;
    int col = blockIdx.x * size * blockDim.x + threadIdx.x * size;

    float sum[size][size] = {0.0f};

    // åŒç¼“å†²å…±äº«å†…å­˜å®šä¹‰ - ä¸ºAå’ŒBçŸ©é˜µå„åˆ›å»ºä¸¤ä¸ªç¼“å†²åŒº
    __shared__ float sA[2][bm][bk];
    __shared__ float sB[2][bk][bn];

    float2* B_float2 = reinterpret_cast<float2*>(B);
    
    // å½“å‰ä½¿ç”¨çš„ç¼“å†²åŒºç´¢å¼•
    int bufferIdx = 0;
    
    // é¢„åŠ è½½ç¬¬ä¸€ä¸ªæ•°æ®å—åˆ°ç¬¬ä¸€ä¸ªç¼“å†²åŒº
    if (row < M && threadIdx.x < bk) {
        sA[0][tidy + 0][threadIdx.x] = A[(row + 0) * K + threadIdx.x];
        sA[0][tidy + 1][threadIdx.x] = A[(row + 1) * K + threadIdx.x];
    }

    if (threadIdx.y < bk && col < N) {
        float2 b_vec = B_float2[(threadIdx.y * N + col) / 2];
        sB[0][threadIdx.y][tidx + 0] = b_vec.x;
        sB[0][threadIdx.y][tidx + 1] = b_vec.y;
    }
    
    __syncthreads();

    // ä¸»å¾ªç¯ - ä½¿ç”¨åŒç¼“å†²ç­–ç•¥
    for (int bk_id = 0; bk_id < K; bk_id += bk) {   
        // ä¸‹ä¸€ä¸ªç¼“å†²åŒºç´¢å¼•
        int nextBufferIdx = 1 - bufferIdx;
        
        // å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ•°æ®å—ï¼Œé¢„åŠ è½½ä¸‹ä¸€ä¸ªæ•°æ®å—åˆ°å¦ä¸€ä¸ªç¼“å†²åŒº
        if (bk_id + bk < K) {
            if (row < M && threadIdx.x < bk) {
                sA[nextBufferIdx][tidy + 0][threadIdx.x] = A[(row + 0) * K + (bk_id + bk) + threadIdx.x];
                sA[nextBufferIdx][tidy + 1][threadIdx.x] = A[(row + 1) * K + (bk_id + bk) + threadIdx.x];
            }

            if (threadIdx.y < bk && col < N) {
                float2 b_vec = B_float2[((bk_id + bk + threadIdx.y) * N + col) / 2];
                sB[nextBufferIdx][threadIdx.y][tidx + 0] = b_vec.x;
                sB[nextBufferIdx][threadIdx.y][tidx + 1] = b_vec.y;
            }
        }

        // ä½¿ç”¨å½“å‰ç¼“å†²åŒºè¿›è¡Œè®¡ç®—
        #pragma unroll
        for (int k = 0; k < bk; ++k) {
            float a0 = sA[bufferIdx][tidy + 0][k];
            float a1 = sA[bufferIdx][tidy + 1][k];

            float2 b_reg = make_float2(sB[bufferIdx][k][tidx + 0],
                                      sB[bufferIdx][k][tidx + 1]);

            sum[0][0] += a0 * b_reg.x;
            sum[0][1] += a0 * b_reg.y;
            sum[1][0] += a1 * b_reg.x;
            sum[1][1] += a1 * b_reg.y;
        }
        
        // åˆ‡æ¢ç¼“å†²åŒº
        bufferIdx = nextBufferIdx;
        
        // åŒæ­¥ä»¥ç¡®ä¿ä¸‹ä¸€ä¸ªæ•°æ®å—å·²å®Œå…¨åŠ è½½
        __syncthreads();
    }

    if ((row + 0) < M && (col+0) < N) C[(row + 0) * N + (col+0)] = sum[0][0];
    if ((row + 0) < M && (col+1) < N) C[(row + 0) * N + (col+1)] = sum[0][1];
    if ((row + 1) < M && (col+0) < N) C[(row + 1) * N + (col+0)] = sum[1][0];
    if ((row + 1) < M && (col+1) < N) C[(row + 1) * N + (col+1)] = sum[1][1];
}
```
![](Figure/gemm/doubleBuffer.png)
- åŒç¼“å†²
   - __shared__ float sA[2][bm][bk];<br>
    __shared__ float sB[2][bk][bn]; <br>
    ä½¿ç”¨ä¸¤ä¸ª buffer æ¥é¢„åŠ è½½ä¸‹ä¸€å—æ•°æ®ï¼Œé¿å…ç­‰å¾…æ—¶é—´
- é¢„åŠ è½½
    - é¢„åŠ è½½ç¬¬ä¸€å—æ•°æ®åˆ°å…±äº«å†…å­˜
    - ä½¿ç”¨ float2 å‘é‡åŒ–æ–¹å¼è¯»å– B çŸ©é˜µï¼Œæ¯æ¬¡è¯»å–ä¸¤ä¸ªæµ®ç‚¹æ•°
- åˆ†å—è®¡ç®—
    - é¢„åŠ è½½ä¸‹ä¸€ä¸ªæ•°æ®å—è‡³å¦ä¸€ä¸ªç¼“å†²åŒº
    - å½“å‰ç¼“å†²åŒºè¿›è¡ŒçŸ©é˜µä¹˜è®¡ç®—
    - åˆ‡æ¢ç¼“å†²åŒº
### æ€§èƒ½åˆ†æ
å–çŸ©é˜µBéœ€è¦å–ä¸€ä¸ªåˆ—å‘é‡ï¼Œè€ŒçŸ©é˜µBåœ¨Shared Memoryä¸­æ˜¯æŒ‰è¡Œå­˜å‚¨çš„ï¼ŒåŠ è½½çš„è¿™ä¸ªåˆ—å‘é‡ä¸Šçš„æ•°æ®ï¼Œå…¨åœ¨ä¸€ä¸ªbankä¸Šï¼Œæ‰€ä»¥ä¼šå¯¼è‡´bank conflictã€‚
![](Figure/gemm/bank%20conflict1.jpg)
## V6. å…±äº«å†…å­˜bankå†²çªä¼˜åŒ–
bank conflictæ˜¯é’ˆå¯¹ä¸€ä¸ªwarpå†…çš„threadså®šä¹‰çš„ã€‚ä¸åŒçš„warpé—´ä¸å­˜åœ¨bank conflictè¿™ä¸ªæ¦‚å¿µã€‚
ä¸‹é¢æ˜¯ä¸¤ä¸ªå®ç°ï¼Œç¬¬ä¸€ä¸ªæ˜¯ä¸v5ä¸€æ ·çš„tile sizeå’Œå¯„å­˜å™¨å¤ç”¨ï¼Œç¬¬äºŒä¸ªå¥½ä¼¼æ›´å¤§çš„tile sizeå’Œæ›´å¤šçš„å¯„å­˜å™¨å¤ç”¨ã€‚
```cpp
// æ·»åŠ paddingä»¥é¿å…bankå†²çª
#define PADDING 1

// ç¡®ä¿å…±äº«å†…å­˜ä¸è¶…è¿‡é™åˆ¶ (ç°åœ¨éœ€è¦ä¸¤å€çš„å…±äº«å†…å­˜ç©ºé—´ç”¨äºåŒç¼“å†²)
static_assert(2 * (bm * (bk+PADDING) + (bk+PADDING) * bn) * sizeof(float) <= 48 * 1024, "share memory is out of 48KB!");

__global__ void matrixMulGPU(float* A, float* B, float* C, int M, int N, int K) { 

    int tidx = threadIdx.x * size;
    int tidy = threadIdx.y * size; 
    int row = blockIdx.y * size * blockDim.y + threadIdx.y * size;
    int col = blockIdx.x * size * blockDim.x + threadIdx.x * size;

    float sum[size][size] = {0.0f};

    // åŒç¼“å†²å…±äº«å†…å­˜å®šä¹‰ - ä¸ºAå’ŒBçŸ©é˜µå„åˆ›å»ºä¸¤ä¸ªç¼“å†²åŒºï¼Œæ·»åŠ paddingä»¥é¿å…bankå†²çª
    __shared__ float sA[2][bm][bk+PADDING];
    __shared__ float sB[2][bk][bn+PADDING];

    float2* B_float2 = reinterpret_cast<float2*>(B);
    
    // å½“å‰ä½¿ç”¨çš„ç¼“å†²åŒºç´¢å¼•
    int bufferIdx = 0;
    
    // é¢„åŠ è½½ç¬¬ä¸€ä¸ªæ•°æ®å—åˆ°ç¬¬ä¸€ä¸ªç¼“å†²åŒº
    if (row < M && threadIdx.x < bk) {
        sA[0][tidy + 0][threadIdx.x] = A[(row + 0) * K + threadIdx.x];
        sA[0][tidy + 1][threadIdx.x] = A[(row + 1) * K + threadIdx.x];
    }

    if (threadIdx.y < bk && col < N) {
        float2 b_vec = B_float2[(threadIdx.y * N + col) / 2];
        sB[0][threadIdx.y][tidx + 0] = b_vec.x;
        sB[0][threadIdx.y][tidx + 1] = b_vec.y;
    }
    
    __syncthreads();

    // ä¸»å¾ªç¯ - ä½¿ç”¨åŒç¼“å†²ç­–ç•¥
    for (int bk_id = 0; bk_id < K; bk_id += bk) {   
        // ä¸‹ä¸€ä¸ªç¼“å†²åŒºç´¢å¼•
        int nextBufferIdx = 1 - bufferIdx;
        
        // å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ•°æ®å—ï¼Œé¢„åŠ è½½ä¸‹ä¸€ä¸ªæ•°æ®å—åˆ°å¦ä¸€ä¸ªç¼“å†²åŒº
        if (bk_id + bk < K) {
            if (row < M && threadIdx.x < bk) {
                sA[nextBufferIdx][tidy + 0][threadIdx.x] = A[(row + 0) * K + (bk_id + bk) + threadIdx.x];
                sA[nextBufferIdx][tidy + 1][threadIdx.x] = A[(row + 1) * K + (bk_id + bk) + threadIdx.x];
            }

            if (threadIdx.y < bk && col < N) {
                float2 b_vec = B_float2[((bk_id + bk + threadIdx.y) * N + col) / 2];
                sB[nextBufferIdx][threadIdx.y][tidx + 0] = b_vec.x;
                sB[nextBufferIdx][threadIdx.y][tidx + 1] = b_vec.y;
            }
        }

        // ä½¿ç”¨å½“å‰ç¼“å†²åŒºè¿›è¡Œè®¡ç®—
        #pragma unroll
        for (int k = 0; k < bk; ++k) {
            float a0 = sA[bufferIdx][tidy + 0][k];
            float a1 = sA[bufferIdx][tidy + 1][k];

            float2 b_reg = make_float2(sB[bufferIdx][k][tidx + 0],
                                      sB[bufferIdx][k][tidx + 1]);

            sum[0][0] += a0 * b_reg.x;
            sum[0][1] += a0 * b_reg.y;
            sum[1][0] += a1 * b_reg.x;
            sum[1][1] += a1 * b_reg.y;
        }
        
        // åˆ‡æ¢ç¼“å†²åŒº
        bufferIdx = nextBufferIdx;
        
        // åŒæ­¥ä»¥ç¡®ä¿ä¸‹ä¸€ä¸ªæ•°æ®å—å·²å®Œå…¨åŠ è½½
        __syncthreads();
    }

    if ((row + 0) < M && (col+0) < N) C[(row + 0) * N + (col+0)] = sum[0][0];
    if ((row + 0) < M && (col+1) < N) C[(row + 0) * N + (col+1)] = sum[0][1];
    if ((row + 1) < M && (col+0) < N) C[(row + 1) * N + (col+0)] = sum[1][0];
    if ((row + 1) < M && (col+1) < N) C[(row + 1) * N + (col+1)] = sum[1][1];
}

// æ·»åŠ ä¸€ä¸ªæ–°çš„ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨æ›´å¤§çš„tile sizeå’Œæ›´å¤šçš„å¯„å­˜å™¨å¤ç”¨
__global__ void matrixMulGPU_optimized(float* A, float* B, float* C, int M, int N, int K) { 

    int tidx = threadIdx.x * size;
    int tidy = threadIdx.y * size;
    int row = blockIdx.y * size * blockDim.y + threadIdx.y * size;
    int col = blockIdx.x * size * blockDim.x + threadIdx.x * size;

    // ä½¿ç”¨å¯„å­˜å™¨æ•°ç»„å­˜å‚¨è®¡ç®—ç»“æœ
    float sum[size][size] = {0.0f};
    float reg_a[size];
    float2 reg_b;

    // åŒç¼“å†²å…±äº«å†…å­˜å®šä¹‰ - ä¸ºAå’ŒBçŸ©é˜µå„åˆ›å»ºä¸¤ä¸ªç¼“å†²åŒºï¼Œæ·»åŠ paddingä»¥é¿å…bankå†²çª
    __shared__ float sA[2][bm][bk+PADDING];
    __shared__ float sB[2][bk][bn+PADDING];

    float2* B_float2 = reinterpret_cast<float2*>(B);
    
    // å½“å‰ä½¿ç”¨çš„ç¼“å†²åŒºç´¢å¼•
    int bufferIdx = 0;
    
    // é¢„åŠ è½½ç¬¬ä¸€ä¸ªæ•°æ®å—åˆ°ç¬¬ä¸€ä¸ªç¼“å†²åŒº
    if (row < M && threadIdx.x < bk) {
        sA[0][tidy + 0][threadIdx.x] = A[(row + 0) * K + threadIdx.x];
        sA[0][tidy + 1][threadIdx.x] = A[(row + 1) * K + threadIdx.x];
    }

    if (threadIdx.y < bk && col < N) {
        float2 b_vec = B_float2[(threadIdx.y * N + col) / 2];
        sB[0][threadIdx.y][tidx + 0] = b_vec.x;
        sB[0][threadIdx.y][tidx + 1] = b_vec.y;
    }
    
    __syncthreads();

    // ä¸»å¾ªç¯ - ä½¿ç”¨åŒç¼“å†²ç­–ç•¥
    for (int bk_id = 0; bk_id < K; bk_id += bk) {   
        // ä¸‹ä¸€ä¸ªç¼“å†²åŒºç´¢å¼•
        int nextBufferIdx = 1 - bufferIdx;
        
        // å¦‚æœä¸æ˜¯æœ€åä¸€ä¸ªæ•°æ®å—ï¼Œé¢„åŠ è½½ä¸‹ä¸€ä¸ªæ•°æ®å—åˆ°å¦ä¸€ä¸ªç¼“å†²åŒº
        if (bk_id + bk < K) {
            if (row < M && threadIdx.x < bk) {
                sA[nextBufferIdx][tidy + 0][threadIdx.x] = A[(row + 0) * K + (bk_id + bk) + threadIdx.x];
                sA[nextBufferIdx][tidy + 1][threadIdx.x] = A[(row + 1) * K + (bk_id + bk) + threadIdx.x];
            }

            if (threadIdx.y < bk && col < N) {
                float2 b_vec = B_float2[((bk_id + bk + threadIdx.y) * N + col) / 2];
                sB[nextBufferIdx][threadIdx.y][tidx + 0] = b_vec.x;
                sB[nextBufferIdx][threadIdx.y][tidx + 1] = b_vec.y;
            }
        }

        // ä½¿ç”¨å½“å‰ç¼“å†²åŒºè¿›è¡Œè®¡ç®—ï¼Œå¢åŠ å¯„å­˜å™¨å¤ç”¨
        #pragma unroll
        for (int k = 0; k < bk; ++k) {
            // é¢„åŠ è½½æ•°æ®åˆ°å¯„å­˜å™¨
            reg_a[0] = sA[bufferIdx][tidy + 0][k];
            reg_a[1] = sA[bufferIdx][tidy + 1][k];
            reg_b = make_float2(sB[bufferIdx][k][tidx + 0], sB[bufferIdx][k][tidx + 1]);

            // è®¡ç®—çŸ©é˜µä¹˜æ³•
            sum[0][0] += reg_a[0] * reg_b.x;
            sum[0][1] += reg_a[0] * reg_b.y;
            sum[1][0] += reg_a[1] * reg_b.x;
            sum[1][1] += reg_a[1] * reg_b.y;
        }
        
        // åˆ‡æ¢ç¼“å†²åŒº
        bufferIdx = nextBufferIdx;
        
        // åŒæ­¥ä»¥ç¡®ä¿ä¸‹ä¸€ä¸ªæ•°æ®å—å·²å®Œå…¨åŠ è½½
        __syncthreads();
    }

    // å†™å›ç»“æœ
    if ((row + 0) < M && (col+0) < N) C[(row + 0) * N + (col+0)] = sum[0][0];
    if ((row + 0) < M && (col+1) < N) C[(row + 0) * N + (col+1)] = sum[0][1];
    if ((row + 1) < M && (col+0) < N) C[(row + 1) * N + (col+0)] = sum[1][0];
    if ((row + 1) < M && (col+1) < N) C[(row + 1) * N + (col+1)] = sum[1][1];
}
```
## cuBlas
```cpp
/////////////gemmçš„gpuå®ç°(cublasç‰ˆæœ¬)////////////////////
//ç¼–è¯‘æ—¶éœ€è¦é“¾æ¥cublasåº“ nvcc -o cublas gemmcublas.cu -lcublas
#include <cuda_runtime.h>
#include <iostream>
#include <cstdlib>
#include <cublas_v2.h> //å¼•å…¥cublasè®¡ç®—åº“

const int M = 2048;
const int N = 2048;
const int K = 2048;


int main() {
    float *h_A = new float[M * K];
    float *h_B = new float[K * N];
    float *h_C = new float[M * N];
    
    for (int i = 0; i < M * K; ++i) {
        h_A[i] = i;
    }
    for (int i = 0; i < K * N; ++i) {
        h_B[i] = i;
    }

    float *d_A, *d_B, *d_C;
    cudaMalloc(&d_A, M * K * sizeof(float));
    cudaMalloc(&d_B, K * N * sizeof(float));
    cudaMalloc(&d_C, M * N * sizeof(float));

    cudaMemcpy(d_A, h_A, M * K * sizeof(float), cudaMemcpyHostToDevice);
    cudaMemcpy(d_B, h_B, K * N * sizeof(float), cudaMemcpyHostToDevice);

    // åˆå§‹åŒ–cuBLAS
    cublasHandle_t handle; //è¿™æ˜¯cuBLASåº“çš„ä¸Šä¸‹æ–‡å¥æŸ„ï¼Œç”¨äºç®¡ç†èµ„æºï¼ˆå¦‚æµã€å†…å­˜ï¼‰å’ŒçŠ¶æ€ï¼ˆå¦‚æ•°å­¦æ¨¡å¼ï¼‰ã€‚æ‰€æœ‰cuBLASå‡½æ•°è°ƒç”¨éƒ½éœ€è¦é€šè¿‡æ­¤å¥æŸ„ã€‚
    cublasCreate(&handle); //åˆå§‹åŒ–cuBLASåº“ï¼Œåˆ†é…èµ„æºå¹¶è¿”å›ä¸€ä¸ªå¥æŸ„ã€‚å›ºå®šå†™æ³• ï¼Œé€šå¸¸åœ¨ç¨‹åºå¼€å§‹æ—¶è°ƒç”¨ä¸€æ¬¡ã€‚

    // è®¾ç½®alphaå’Œbetaå‚æ•°ï¼ˆC = alpha * A * B + beta * Cï¼‰
    const float alpha = 1.0f;
    const float beta = 0.0f;

    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(start, 0);

    /////////////////////////////////////////cubalsSgemmå‡½æ•°åŸå‹/////////////////////////////////////
    //     cublasStatus_t cublasSgemm(
    //     cublasHandle_t handle,                                //1.ä¸Šä¸‹æ–‡å¥æŸ„
    //     cublasOperation_t transa, cublasOperation_t transb,   //2.3.æ˜¯å¦è½¬ç½®çŸ©é˜µAå’ŒBï¼šCUBLAS_OP_N=ä¸è½¬ç½®ï¼ŒCUBLAS_OP_T=è½¬ç½®
    //     int m, int n, int k,                                  //4.5.6. m n kåˆ†é…å¯¹åº” C(m,n) = alpha * A(m,k) * B(k,n) + beta * C(m,n)
    //     const float *alpha,                                   //7. ç¼©æ”¾å› å­alpha
    //     const float *A, int lda,  //8.çŸ©é˜µA(m,k)(å¿…é¡»ä¸ºæŒ‡å‘GPUå†…å­˜çš„æŒ‡é’ˆ), 9.int lda:çŸ©é˜µAçš„ä¸»ç»´åº¦ï¼ˆè¡Œä¼˜å…ˆæ—¶ä¸ºåˆ—æ•°ï¼Œåˆ—ä¼˜å…ˆæ—¶ä¸ºè¡Œæ•°ï¼‰æˆ‘ä»¬è¿™é‡Œè¾“å…¥K
    //     const float *B, int ldb,  //10.çŸ©é˜µB(k,n)  11.int ldb:çŸ©é˜µBçš„ä¸»ç»´åº¦,ç”±äºæˆ‘ä»¬æƒ³è®©å®ƒä»¥è¡Œä¼˜å…ˆå­˜å‚¨ï¼Œå› æ­¤è¿™é‡Œè¾“å…¥å®ƒçš„åˆ—æ•° N
    //     const float *beta,                                    //12. ç¼©æ”¾å› å­beta
    //     float *C, int ldc         //13.çŸ©é˜µC(m,n)  14.int ldc:çŸ©é˜µCçš„ä¸»ç»´åº¦,ç”±äºæˆ‘ä»¬æƒ³è®©å®ƒä»¥è¡Œä¼˜å…ˆå­˜å‚¨ï¼Œå› æ­¤è¿™é‡Œè¾“å…¥å®ƒçš„åˆ—æ•° N
    //      );
    /////////////////////////////////////////cubalsSgemmå‡½æ•°åŸå‹/////////////////////////////////////

    //æ³¨æ„ï¼šcuBLASå‡è®¾çŸ©é˜µæŒ‰åˆ—ä¼˜å…ˆå­˜å‚¨ï¼å¦‚æœä»£ç ä¸­æ˜¯è¡Œä¼˜å…ˆçš„æ•°æ®(ä¾‹å¦‚C/C++)ï¼Œåˆ™å¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹å¼è½¬æ¢cuBLASä¸ºè¡Œä¼˜å…ˆ
    //     1.äº¤æ¢çŸ©é˜µAå’ŒBçš„é¡ºåºå¹¶è½¬ç½®ã€‚ï¼ˆè¿™ä¸ªæ–¹æ³•æœ‰ç‚¹éº»çƒ¦ï¼‰
    //     2.è°ƒæ•´ä¸»ç»´åº¦ï¼ˆlda/ldb/ldcï¼‰ä¸»ç»´åº¦æ˜¯çŸ©é˜µåœ¨å†…å­˜ä¸­çš„è¡Œæ•°ï¼ˆè¡Œä¼˜å…ˆï¼‰æˆ–åˆ—æ•°ï¼ˆåˆ—ä¼˜å…ˆï¼‰ã€‚
    ////æˆ‘ä»¬è¿™ä¸ªä»£ç ä¸­æ˜¯é‡‡ç”¨çš„æ–¹æ³•2ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼


    // è°ƒç”¨cubalsgemmå‡½æ•°
    for (int i = 0; i < 200; ++i){
        cublasSgemm(handle, CUBLAS_OP_N, CUBLAS_OP_N,
            M, N, K, &alpha, d_A, K, d_B, N, &beta, d_C, N);
    }

    cudaEventRecord(stop, 0);
    cudaEventSynchronize(stop);
    float milliseconds = 0;
    cudaEventElapsedTime(&milliseconds, start, stop);
    milliseconds /= 200;
    std::cout << "cuBlLASçŸ©é˜µä¹˜æ³•æ—¶é—´: " << milliseconds << "æ¯«ç§’" << std::endl;

    double flops = 2.0 * M * N * K;
    double gflops = flops / (milliseconds/1000 * 1e9);
    std::cout << "GFLOPS: " << gflops << std::endl;

    cudaMemcpy(h_C, d_C, M * N * sizeof(float), cudaMemcpyDeviceToHost);
    std::cout << "C[0][0] = " << h_C[0] << std::endl;
    
    // æ¸…ç†æ‰cuBLAS
    cublasDestroy(handle); // é‡Šæ”¾cublasèµ„æºï¼ˆç¨‹åºç»“æŸå‰è°ƒç”¨ï¼‰

    cudaFree(d_A);
    cudaFree(d_B);
    cudaFree(d_C);

    delete[] h_A;
    delete[] h_B;
    delete[] h_C;

    return 0;
    // cuBlLASçŸ©é˜µä¹˜æ³•æ—¶é—´: 1.31482æ¯«ç§’
    // GFLOPS: 13066.4
    // C[0][0] = 5.85977e+12
}
```
- ä½¿ç”¨cublasæ—¶æ²¡æœ‰å¯¹çŸ©é˜µè¿›è¡Œè½¬ç½®ï¼Œè½¬ç½®åæ€§èƒ½æ¯”è¿™ä¸ªç¨å·®
    - åŸå› ï¼š1. cuBLAS ä»¥åˆ—ä¸ºä¸»è®¿å­˜ï¼Œå¯¼è‡´è®¿å­˜ä¸è¿ç»­ï¼ˆé coalescedï¼‰ï¼Œæ€§èƒ½ä¸‹é™<br>
          2. GPU L2/L1 cache å‘½ä¸­ç‡ä¸‹é™<br>
           3. warp çº§åˆ«çš„è®¿å­˜æœªå¯¹é½

 
## æ€§èƒ½å¯¹æ¯”
![](Figure/gemm/æ€§èƒ½å¯¹æ¯”.jpg)
1. [https://zhuanlan.zhihu.com/p/410278370] -CUDA çŸ©é˜µä¹˜æ³•ç»ˆæä¼˜åŒ–æŒ‡å—
2. [https://zhuanlan.zhihu.com/p/435908830] - æ·±å…¥æµ…å‡ºGPUä¼˜åŒ–ç³»åˆ—ï¼šGEMMä¼˜åŒ–(ä¸€)
3. [https://zhuanlan.zhihu.com/p/703256080] -ä»å•¥ä¹Ÿä¸ä¼šåˆ°CUDA GEMMä¼˜åŒ–
4. [https://blog.csdn.net/LostUnravel/article/details/138034380] -[CUDA å­¦ä¹ ç¬”è®°] å¦‚ä½•ä¼˜åŒ– CUDA çŸ©é˜µä¹˜å†…æ ¸ä»¥è·å¾—ç±»ä¼¼ cuBLAS çš„æ€§èƒ½: å·¥ä½œæ—¥å¿—