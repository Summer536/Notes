"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[7812],{2013:(e,t,a)=>{a.r(t),a.d(t,{comp:()=>d,data:()=>h});var n=a(641);const o=a.p+"assets/img/1.65dc6465.png",r=a.p+"assets/img/pagedattention_workflow.ca33619d.gif",i=a.p+"assets/img/2.90d9a456.png",c=a.p+"assets/img/multiple_outputs.8470e5cc.gif",p=a.p+"assets/img/3.77d1c3ee.png",s=a.p+"assets/img/4.50244e62.png",l=a.p+"assets/img/5.4c271e05.png",g={},d=(0,a(6262).A)(g,[["render",function(e,t){return(0,n.uX)(),(0,n.CE)("div",null,[t[0]||(t[0]=(0,n.Lk)("h1",{id:"pagedattention",tabindex:"-1"},[(0,n.Lk)("a",{class:"header-anchor",href:"#pagedattention"},[(0,n.Lk)("span",null,"Pagedattention")])],-1)),t[1]||(t[1]=(0,n.Lk)("h2",{id:"简介",tabindex:"-1"},[(0,n.Lk)("a",{class:"header-anchor",href:"#简介"},[(0,n.Lk)("span",null,"简介")])],-1)),t[2]||(t[2]=(0,n.Lk)("p",null,[(0,n.eW)("PagedAttention是对kv cache所占空间的分页管理，是一个典型的"),(0,n.Lk)("strong",null,"以内存空间换计算开销"),(0,n.eW)("的手段，vllm和tenorRT-llm都应用了这个手段来节约kv cache占用的memory，和现今大模型训练的recompute中间activation用于bwd的以计算开销换内存空间的手段恰好相反。")],-1)),(0,n.Q3)(" more "),t[3]||(t[3]=(0,n.Fv)('<h1 id="一、背景" tabindex="-1"><a class="header-anchor" href="#一、背景"><span>一、背景</span></a></h1><h2 id="_1-kv-cache" tabindex="-1"><a class="header-anchor" href="#_1-kv-cache"><span>1. kv cache</span></a></h2><p>PagedAttention主要是对kv cache所占空间的分页管理，因此本文有必要先来简单介绍一下kv cache。</p><ol><li><p><strong>kv cache的来源</strong>：<br> decoder推理中，对于每个输入的 prompt，在计算第一个 token 输出的时候，每个 token 的 attention 肯定是都要从头计算, 但是在后续 token 的生成中，<strong>需要concat前面每一个 token 的 K 和 V</strong>，由于模型参数矩阵是不变的，此时只有刚生成的那个 token 的 K 和 V 需要从头计算，所以可以把之前token的K和V缓存起来避免重复计算，这个就叫kv cache。</p></li><li><p><strong>kv cache的占用的显存空间</strong>：<br> 每个decoder layer，每个 token 的 K、V 矩阵都是 embedding_size=num_heads * head_size，要计算总的Token需要再乘上 seqlen和 batch size，就可以计算出每个layer的 kv Cache 所需的存储容量了。例如，如果 batch size = 8，在 LLaMA 2-70B 中，80 层layer的 KV Cache 一共需要 80 * 8192 * 4096 * 8 * 2Byte = 40 GB。相比 LLaMA 2-70B(fp16)的140 GB 的参数量，其实还好。</p></li><li><p><strong>kv cache节省的FLOPS</strong>：<br> 每个token的 K、V 矩阵计算一共需要 2 (K+V) * 2 (mul+add) * embedding size * embedding size = 4 * 8192 * 8192 这么多计算量，乘以seqlen、num_layer和 batch size，一共省了 4096 * 80 * 8 * 4 * 8192 * 8192 = 640 TFLOPs的计算量，当然，因seqlen和embedding size和num layer而异。</p></li><li><p><strong>kv cache读取的weight大小和读取时间</strong>：<br> K=input乘Wk，V=input乘Wv，我们还需要去显存中读取这两个linear的weight，weight的shape为[batch size, seqlen, embedding size, embedding size]，还是带入以上的取值，那么这<strong>两个weight的参数量为4096 * 80 * 2 * 8192 * 8192</strong> , 查阅A100和H100的显存带宽可以知道，已经是最先进的HBM了，不是老的GDDR了，A100 HBM带宽为2 TB/s，H100 HBM带宽为3.35 TB/s，那么带宽/参数大小就是读取时间，<strong>大约有几十秒，这显然延迟太高了</strong>，还不说每次token generation都要去读然后来计算K V，所以kv cache非常有必要，即使占了很大显存都要用。</p></li></ol><h2 id="_2-新的挑战" tabindex="-1"><a class="header-anchor" href="#_2-新的挑战"><span>2. 新的挑战</span></a></h2><p>kvcache的出现，确实节省了decoder阶段的计算量，加快了其推理速度，但是也带来了新的挑战。kv cache所占的空间也确实是大且有浪费的。</p><p>KV cache有它独特的地方：<strong>它在解码时会动态变化，并且输出长度和生命周期也是不能提前知道的。这些特性导致现存系统的如下问题</strong>：</p><p>首先，<strong>现存系统会存在大量内部(internal)和外部(external)碎片(fragment)</strong>。为了把KV cache存放在连续的空间，这些系统会为一个请求提前分配(pre-allocate)最大可能长度（max seq len）的cache，这会导致内部碎片，因为实际长度可能远远小于最大长度。而这些内部内存碎片白白浪费了，只有等等这个请求结束才能释放。而且即使我们可以提前预料到生成结果的长度，比如512。我们在一开始(比较解码第一个输出token)就预留了512个token的cache空间，这些空间也不能被其它请求使用。此外，<strong>由于每个输入请求的长度不同，也会导致大量的外部碎片</strong>。下图显示了在现存系统中，只有20.4% - 38.2%的KV cache内存是真正被用于存储有用的token。</p><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>那么，对症下药，解决办法主要集中在两个方面：</p><ol><li>KV cache不一定必须存放在连续的空间；</li><li>KV cache不一定必须按照max seq len来申请，可以动态的根据当前senlen的长度来定；</li></ol><h1 id="二、pagedattention思想" tabindex="-1"><a class="header-anchor" href="#二、pagedattention思想"><span>二、PagedAttention思想</span></a></h1><h2 id="_1-pagedattention方案详解" tabindex="-1"><a class="header-anchor" href="#_1-pagedattention方案详解"><span>1. PagedAttention方案详解</span></a></h2><p>受操作系统中虚拟内存和分页机制启发，vLLM 提出了 PagedAttention 注意力算法，以实现 KV Cache 的动态内存分配，而不是像之前一样为每个 seq 都分配固定大小的 [max_seq_len, hidden_size] 连续内存空间用于存储 kv cache。</p><p>具体来说，PagedAttention <strong>将每个序列从逻辑上划分为一定数量的 blocks（块），每个 block 包含每个 seq 一定数量 tokens 的 key 和 value</strong>，并把这些<strong>逻辑 blocks 通过 block table 映射到固定大小的 物理 blocks 上</strong>，物理 blocks 可能不连续，即 kv 可能不连续分布。一句话总结就是构建 blocks 表， 并将 seq 的 kv tokens 划分成逻辑 blocks 并映射到物理 blocks 上。</p><p>从kernel角度看：pagedattention CUDA kernel通过block table拿到对应的physical block序号，然后CUDA线程ID计算每个seq每个token的offset从而fetch相应的block，拿到kv，继续做attention的计算</p><p>使用 PagedAttention 的请求的生成过程如下图所示：</p><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>这种方式带来的<strong>内存浪费仅出现在序列的最后一个块中</strong>，实际中带来了近乎最优的内存使用，浪费不到 4%。这种内存效率的提升大大提高了系统能够同时处理的序列数量，增加了 GPU 的利用率，并显著提升了处理吞吐量。</p><p>PagedAttention 这种结构类似于操作系统中的虚拟内存，其中将块视为页，将 tokens 视为字节，将序列视为进程。序列的逻辑连续块通过块表映射到非连续的物理块中。当新的 tokens 被生成时，这些物理块会按需分配。</p><p><strong>多请求序列解码情况：</strong></p><p>上面的例子演示的是一个请求的解码过程。<strong>实际的情况是每一步vLLM都会从后续队列里选择一些请求来batching，并且为新的逻辑block分配物理block。然后把多个请求的prompt和最近生成的tokens拼成一个大的sequence给到vLLM的kernel</strong>(GPU的kernel)。这个实现了PagedAttention算法的kernel会访问这些逻辑block的cache并且为每一个请求都生成一个新的token，并且把这一步的KV cache也保存到物理block里。如果block大小越大，那么这个kernel读取的次数就会越小，但是内部碎片也会越大。</p><figure><img src="'+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>在上图的例子里，有两个请求。我们可以看到<strong>两个逻辑相邻的block物理上并不需要相邻。相反，两个请求最后一个物理块(3和2)是相邻的</strong>，这反而可以让kernel的效率更高（因为kernel的读取只是最后生成的Token，之前的不管，这些不同序列生成的最后Token对应的不同Block（如上图的Block2和Block3）在内存上连续的）。</p><h2 id="_2-pagedattention的内存共享优势" tabindex="-1"><a class="header-anchor" href="#_2-pagedattention的内存共享优势"><span>2. PagedAttention的内存共享优势</span></a></h2><p>PagedAttention 借助块表实现了灵活的内存共享机制。类似于进程间共享物理页面的方式，PagedAttention 中的不同序列可以通过将各自的逻辑块映射到相同的物理块来共享内存资源。为了确保共享的安全性，PagedAttention 跟踪物理块的引用次数，并采用<strong>写时复制（copy-on-write）策略</strong>以防止数据冲突。</p><figure><img src="'+c+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="并行采样-parallel-sampling" tabindex="-1"><a class="header-anchor" href="#并行采样-parallel-sampling"><span>并行采样(Parallel sampling)</span></a></h3><p>在代码生成等常见，为了结果的多样性，对于同一个prompt，我们可能会在每一步都随机采样多个(而不是一个)token。</p><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>上图是一个例子，由于<strong>两个结果的prompt是相同的，因此KV cache可以共享</strong>。为了实现共享，我们在block table里的每个block里增加一个引用计数，比如这里的第7个物理block和第1个物理block都映射到两个逻辑block。现在假设第1个sample先执行，那么它需要在物理block1写入token “father”，因为这个物理block被多于1个人用到，所以vLLM把block1复制一份到物理block3，然后修改sample1的映射为block3，然后再把”father”的cache写入，同时减少block1的引用计数。接着第2个sample执行，这个时候block1的引用计数为1，也就是它独享，所以可以放心写入。这就是所谓的Copy On Write机制——也就是多个使用者共享一个资源，<strong>大家可以共享读，但是如果某人要写，那么它就需要Copy一份，然后在它自己的那份上修改</strong>。</p><p>如果Prompt很长，则这种共享会非常有价值。</p><h3 id="beam-search" tabindex="-1"><a class="header-anchor" href="#beam-search"><span>beam search</span></a></h3><p>beam search是在每一个时刻都保留k个(有时候k会变，比如topp，但是不影响原理)最优路径。比如下图：<br><img src="'+s+'" alt="" loading="lazy"></p><p>这里beam size是2，也就是每次保留最好的两条路径。一开始的prompt是相同的，假设是block 0，接着它展开为block 1和block 2，接着展开为3和4，这几步只有2个候选路径，没啥好说。接着block 3展开为block 567，block 4展开为block8，最优的是block 6和7。这个时候我们要<strong>保留路径6和7的KV cache，我们发现它们的路径有很大一部分是重合的(block 013)</strong>。</p><p>前面也说过，beam search的top路径会有很多相似的子路径，因此PagedAttention能够充分利用这一点来提高共享比例。</p><h3 id="共享前缀" tabindex="-1"><a class="header-anchor" href="#共享前缀"><span>共享前缀</span></a></h3><p>在很多应用中，比如In-context learning，我们会增加很长的few-shot examples。比如：</p><p><img src="'+l+'" alt="" loading="lazy"><br> 上面是一个机器翻译的例子，在input之前有很长的前缀。另外包括chatbot，我们也会设置system角色的prompt。这些都是可以共享的。</p><h1 id="三、pagedattention源码解析" tabindex="-1"><a class="header-anchor" href="#三、pagedattention源码解析"><span>三、PagedAttention源码解析</span></a></h1><h1 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h1><ol><li><p><a href="https://arxiv.org/pdf/2309.06180" target="_blank" rel="noopener noreferrer">Efficient Memory Management for Large Language Model Serving with PagedAttention</a></p></li><li><p><a href="https://fancyerii.github.io/2023/11/01/pagedattention/" target="_blank" rel="noopener noreferrer">PagedAttention论文解读</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2ODk4MzE2MQ==&amp;mid=2247483932&amp;idx=1&amp;sn=a2b05f9bbd4bf266b140c6deb7b8d034&amp;chksm=cf208735a0d88c834a705799b8bcc50a765aa12ce3a3beb2ab375d43f680f0339ec23f19d40d&amp;scene=126&amp;sessionid=1747114269#rd" target="_blank" rel="noopener noreferrer">PagedAttention/KV cache--大模型推理服务框架vLLM要点简析</a></p></li><li><p><a href="https://www.armcvai.cn/2024-10-26/vllm-optimize.html" target="_blank" rel="noopener noreferrer">vllm优化技术速览</a></p></li><li><p><a href="https://www.armcvai.cn/2024-11-17/vllm-pagedattention.html" target="_blank" rel="noopener noreferrer">vllm 优化之 PagedAttention 源码解读</a></p></li><li><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2ODk4MzE2MQ==&amp;mid=2247483998&amp;idx=1&amp;sn=edc575269cf34f579303e08cca221d0d&amp;chksm=cfc5789a6f4739a3790d605bc30007c1385eb96946a740eacfe2ca20ae31861b4914b758a5b4&amp;scene=126&amp;sessionid=1747114269#rd" target="_blank" rel="noopener noreferrer">CUDA PagedAttention kernel源码解析--大模型推理服务框架vLLM要点简析（下）</a></p></li></ol>',42))])}]]),h=JSON.parse('{"path":"/zh/posts/pagedattention.html","title":"PagedAttention","lang":"zh-CN","frontmatter":{"title":"PagedAttention","date":"2025-05-23T00:00:00.000Z","readingTime":600,"category":["笔记"],"tag":["GPU优化","大模型","算法优化"],"isOriginal":true,"description":"简介 PagedAttention是对kv cache所占空间的分页管理，是一个典型的以内存空间换计算开销的手段，vllm和tenorRT-llm都应用了这个手段来节约kv cache占用的memory，和现今大模型训练的recompute中间activation用于bwd的以计算开销换内存空间的手段恰好相反。","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"PagedAttention\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-05-23T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-24T04:10:46.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/Summer536\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/posts/pagedattention.html"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"PagedAttention"}],["meta",{"property":"og:description","content":"简介 PagedAttention是对kv cache所占空间的分页管理，是一个典型的以内存空间换计算开销的手段，vllm和tenorRT-llm都应用了这个手段来节约kv cache占用的memory，和现今大模型训练的recompute中间activation用于bwd的以计算开销换内存空间的手段恰好相反。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-24T04:10:46.000Z"}],["meta",{"property":"article:tag","content":"算法优化"}],["meta",{"property":"article:tag","content":"大模型"}],["meta",{"property":"article:tag","content":"GPU优化"}],["meta",{"property":"article:published_time","content":"2025-05-23T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-24T04:10:46.000Z"}]]},"git":{"createdTime":1747832748000,"updatedTime":1748059846000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":3,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":8.86,"words":2658},"filePathRelative":"zh/posts/pagedattention.md","excerpt":"\\n<h2>简介</h2>\\n<p>PagedAttention是对kv cache所占空间的分页管理，是一个典型的<strong>以内存空间换计算开销</strong>的手段，vllm和tenorRT-llm都应用了这个手段来节约kv cache占用的memory，和现今大模型训练的recompute中间activation用于bwd的以计算开销换内存空间的手段恰好相反。</p>\\n","autoDesc":true}')},6262:(e,t)=>{t.A=(e,t)=>{const a=e.__vccOpts||e;for(const[e,n]of t)a[e]=n;return a}}}]);