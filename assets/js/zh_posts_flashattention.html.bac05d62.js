"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[609],{6262:(s,a)=>{a.A=(s,a)=>{const n=s.__vccOpts||s;for(const[s,t]of a)n[s]=t;return n}},8299:(s,a,n)=>{n.r(a),n.d(a,{comp:()=>g,data:()=>d});var t=n(641);const m=n.p+"assets/img/FAV1_0.de162840.png",e=n.p+"assets/img/FAV1_2.92bf628c.png",p=n.p+"assets/img/MACandFlops.6e053a7e.png",l=n.p+"assets/img/FAV1_6.b5674640.png",i=n.p+"assets/img/FAV1_5.3ad82930.png",r=n.p+"assets/img/step1(1).30d8e3e9.png",c=n.p+"assets/img/step2.3cdd4bb5.png",h=n.p+"assets/img/FAV1_9.f393d210.png",o={},g=(0,n(6262).A)(o,[["render",function(s,a){return(0,t.uX)(),(0,t.CE)("div",null,[a[0]||(a[0]=(0,t.Lk)("h1",{id:"flashattention",tabindex:"-1"},[(0,t.Lk)("a",{class:"header-anchor",href:"#flashattention"},[(0,t.Lk)("span",null,"Flashattention")])],-1)),a[1]||(a[1]=(0,t.Lk)("h2",{id:"简介",tabindex:"-1"},[(0,t.Lk)("a",{class:"header-anchor",href:"#简介"},[(0,t.Lk)("span",null,"简介")])],-1)),a[2]||(a[2]=(0,t.Lk)("p",null,"Flashattention是一种高效的注意力机制计算方法，由斯坦福大学研究团队在2022年提出。它通过优化GPU内存访问模式，显著提高了Transformer模型中注意力计算的速度并降低了内存使用。作为大模型训练和推理的关键优化技术，Flashattention已被广泛应用于各种大型语言模型中。",-1)),(0,t.Q3)(" more "),a[3]||(a[3]=(0,t.Fv)('<h2 id="一、flashattention-v1" tabindex="-1"><a class="header-anchor" href="#一、flashattention-v1"><span>一、Flashattention-V1</span></a></h2><h3 id="_1-1-标准注意力机制" tabindex="-1"><a class="header-anchor" href="#_1-1-标准注意力机制"><span>1.1 标准注意力机制</span></a></h3><p>给定输入二维矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">Q, K, V \\in \\mathbb{R}^{N \\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8778em;vertical-align:-0.1944em;"></span><span class="mord mathnormal">Q</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span>，其中 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 是输入序列的长度，<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord mathnormal">d</span></span></span></span> 是自注意力机制头的长度。Softmax 是按行应用的，注意力输出矩阵 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>N</mi><mo>×</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">O \\in \\mathbb{R}^{N \\times d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7224em;vertical-align:-0.0391em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8491em;"></span><span class="mord"><span class="mord mathbb">R</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mbin mtight">×</span><span class="mord mathnormal mtight">d</span></span></span></span></span></span></span></span></span></span></span></span> 的计算公式如下：</p>',3)),a[4]||(a[4]=(0,t.Lk)("p",{class:"katex-block"},[(0,t.Lk)("span",{class:"katex-display"},[(0,t.Lk)("span",{class:"katex"},[(0,t.Lk)("span",{class:"katex-mathml"},[(0,t.Lk)("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[(0,t.Lk)("semantics",null,[(0,t.Lk)("mtable",{rowspacing:"0.25em",columnalign:"right left",columnspacing:"0em"},[(0,t.Lk)("mtr",null,[(0,t.Lk)("mtd",null,[(0,t.Lk)("mstyle",{scriptlevel:"0",displaystyle:"true"},[(0,t.Lk)("mi",null,"S")])]),(0,t.Lk)("mtd",null,[(0,t.Lk)("mstyle",{scriptlevel:"0",displaystyle:"true"},[(0,t.Lk)("mrow",null,[(0,t.Lk)("mrow"),(0,t.Lk)("mo",null,"="),(0,t.Lk)("mi",null,"Q"),(0,t.Lk)("msup",null,[(0,t.Lk)("mi",null,"K"),(0,t.Lk)("mi",{mathvariant:"normal"},"T")]),(0,t.Lk)("mo",null,"∈"),(0,t.Lk)("msup",null,[(0,t.Lk)("mi",{mathvariant:"double-struck"},"R"),(0,t.Lk)("mrow",null,[(0,t.Lk)("mi",null,"N"),(0,t.Lk)("mo",null,"×"),(0,t.Lk)("mi",null,"N")])]),(0,t.Lk)("mo",{separator:"true"},","),(0,t.Lk)("mspace",{width:"1em"}),(0,t.Lk)("mi",null,"P"),(0,t.Lk)("mo",null,"="),(0,t.Lk)("mtext",null,"softmax"),(0,t.Lk)("mo",{stretchy:"false"},"("),(0,t.Lk)("mi",null,"S"),(0,t.Lk)("mo",{stretchy:"false"},")"),(0,t.Lk)("mo",null,"∈"),(0,t.Lk)("msup",null,[(0,t.Lk)("mi",{mathvariant:"double-struck"},"R"),(0,t.Lk)("mrow",null,[(0,t.Lk)("mi",null,"N"),(0,t.Lk)("mo",null,"×"),(0,t.Lk)("mi",null,"N")])]),(0,t.Lk)("mo",{separator:"true"},","),(0,t.Lk)("mspace",{width:"1em"}),(0,t.Lk)("mi",null,"O"),(0,t.Lk)("mo",null,"="),(0,t.Lk)("mi",null,"P"),(0,t.Lk)("mi",null,"V"),(0,t.Lk)("mo",null,"∈"),(0,t.Lk)("msup",null,[(0,t.Lk)("mi",{mathvariant:"double-struck"},"R"),(0,t.Lk)("mrow",null,[(0,t.Lk)("mi",null,"N"),(0,t.Lk)("mo",null,"×"),(0,t.Lk)("mi",null,"d")])]),(0,t.Lk)("mi",{mathvariant:"normal"},".")])])])])]),(0,t.Lk)("annotation",{encoding:"application/x-tex"},"\\begin{align*} S &= Q K^\\mathrm{T} \\in \\mathbb{R}^{N \\times N}, \\quad P = \\text{softmax}(S) \\in \\mathbb{R}^{N \\times N}, \\quad O = P V \\in \\mathbb{R}^{N \\times d}. \\end{align*} ")])])]),(0,t.Lk)("span",{class:"katex-html","aria-hidden":"true"},[(0,t.Lk)("span",{class:"base"},[(0,t.Lk)("span",{class:"strut",style:{height:"1.5591em","vertical-align":"-0.5296em"}}),(0,t.Lk)("span",{class:"mord"},[(0,t.Lk)("span",{class:"mtable"},[(0,t.Lk)("span",{class:"col-align-r"},[(0,t.Lk)("span",{class:"vlist-t vlist-t2"},[(0,t.Lk)("span",{class:"vlist-r"},[(0,t.Lk)("span",{class:"vlist",style:{height:"1.0296em"}},[(0,t.Lk)("span",{style:{top:"-3.1304em"}},[(0,t.Lk)("span",{class:"pstrut",style:{height:"3em"}}),(0,t.Lk)("span",{class:"mord"},[(0,t.Lk)("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S")])])]),(0,t.Lk)("span",{class:"vlist-s"},"​")]),(0,t.Lk)("span",{class:"vlist-r"},[(0,t.Lk)("span",{class:"vlist",style:{height:"0.5296em"}},[(0,t.Lk)("span")])])])]),(0,t.Lk)("span",{class:"col-align-l"},[(0,t.Lk)("span",{class:"vlist-t vlist-t2"},[(0,t.Lk)("span",{class:"vlist-r"},[(0,t.Lk)("span",{class:"vlist",style:{height:"1.0296em"}},[(0,t.Lk)("span",{style:{top:"-3.1304em"}},[(0,t.Lk)("span",{class:"pstrut",style:{height:"3em"}}),(0,t.Lk)("span",{class:"mord"},[(0,t.Lk)("span",{class:"mord"}),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mrel"},"="),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mord mathnormal"},"Q"),(0,t.Lk)("span",{class:"mord"},[(0,t.Lk)("span",{class:"mord mathnormal",style:{"margin-right":"0.07153em"}},"K"),(0,t.Lk)("span",{class:"msupsub"},[(0,t.Lk)("span",{class:"vlist-t"},[(0,t.Lk)("span",{class:"vlist-r"},[(0,t.Lk)("span",{class:"vlist",style:{height:"0.8913em"}},[(0,t.Lk)("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[(0,t.Lk)("span",{class:"pstrut",style:{height:"2.7em"}}),(0,t.Lk)("span",{class:"sizing reset-size6 size3 mtight"},[(0,t.Lk)("span",{class:"mord mathrm mtight"},"T")])])])])])])]),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mrel"},"∈"),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mord"},[(0,t.Lk)("span",{class:"mord mathbb"},"R"),(0,t.Lk)("span",{class:"msupsub"},[(0,t.Lk)("span",{class:"vlist-t"},[(0,t.Lk)("span",{class:"vlist-r"},[(0,t.Lk)("span",{class:"vlist",style:{height:"0.8913em"}},[(0,t.Lk)("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[(0,t.Lk)("span",{class:"pstrut",style:{height:"2.7em"}}),(0,t.Lk)("span",{class:"sizing reset-size6 size3 mtight"},[(0,t.Lk)("span",{class:"mord mtight"},[(0,t.Lk)("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"N"),(0,t.Lk)("span",{class:"mbin mtight"},"×"),(0,t.Lk)("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"N")])])])])])])])]),(0,t.Lk)("span",{class:"mpunct"},","),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"1em"}}),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),(0,t.Lk)("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mrel"},"="),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mord text"},[(0,t.Lk)("span",{class:"mord"},"softmax")]),(0,t.Lk)("span",{class:"mopen"},"("),(0,t.Lk)("span",{class:"mord mathnormal",style:{"margin-right":"0.05764em"}},"S"),(0,t.Lk)("span",{class:"mclose"},")"),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mrel"},"∈"),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mord"},[(0,t.Lk)("span",{class:"mord mathbb"},"R"),(0,t.Lk)("span",{class:"msupsub"},[(0,t.Lk)("span",{class:"vlist-t"},[(0,t.Lk)("span",{class:"vlist-r"},[(0,t.Lk)("span",{class:"vlist",style:{height:"0.8913em"}},[(0,t.Lk)("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[(0,t.Lk)("span",{class:"pstrut",style:{height:"2.7em"}}),(0,t.Lk)("span",{class:"sizing reset-size6 size3 mtight"},[(0,t.Lk)("span",{class:"mord mtight"},[(0,t.Lk)("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"N"),(0,t.Lk)("span",{class:"mbin mtight"},"×"),(0,t.Lk)("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"N")])])])])])])])]),(0,t.Lk)("span",{class:"mpunct"},","),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"1em"}}),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.1667em"}}),(0,t.Lk)("span",{class:"mord mathnormal",style:{"margin-right":"0.02778em"}},"O"),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mrel"},"="),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mord mathnormal",style:{"margin-right":"0.13889em"}},"P"),(0,t.Lk)("span",{class:"mord mathnormal",style:{"margin-right":"0.22222em"}},"V"),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mrel"},"∈"),(0,t.Lk)("span",{class:"mspace",style:{"margin-right":"0.2778em"}}),(0,t.Lk)("span",{class:"mord"},[(0,t.Lk)("span",{class:"mord mathbb"},"R"),(0,t.Lk)("span",{class:"msupsub"},[(0,t.Lk)("span",{class:"vlist-t"},[(0,t.Lk)("span",{class:"vlist-r"},[(0,t.Lk)("span",{class:"vlist",style:{height:"0.8991em"}},[(0,t.Lk)("span",{style:{top:"-3.113em","margin-right":"0.05em"}},[(0,t.Lk)("span",{class:"pstrut",style:{height:"2.7em"}}),(0,t.Lk)("span",{class:"sizing reset-size6 size3 mtight"},[(0,t.Lk)("span",{class:"mord mtight"},[(0,t.Lk)("span",{class:"mord mathnormal mtight",style:{"margin-right":"0.10903em"}},"N"),(0,t.Lk)("span",{class:"mbin mtight"},"×"),(0,t.Lk)("span",{class:"mord mathnormal mtight"},"d")])])])])])])])]),(0,t.Lk)("span",{class:"mord"},".")])])]),(0,t.Lk)("span",{class:"vlist-s"},"​")]),(0,t.Lk)("span",{class:"vlist-r"},[(0,t.Lk)("span",{class:"vlist",style:{height:"0.5296em"}},[(0,t.Lk)("span")])])])])])])])])])])],-1)),a[5]||(a[5]=(0,t.Fv)('<p><img src="'+m+'" alt="" loading="lazy"><br> 标准的 Attention 运算大致可以描述为以下三个步骤：</p><ul><li>将 Q,K 矩阵以块的形式从 HBM 中加载到 SRAM 中，计算 S=QK ，将 S 写入到 HBM 中。</li><li>将 S 矩阵从 HBM 中加载到 SRAM 中，计算 P=Softmax(S) ，将 P 写入到 HBM 中。</li><li>将 P,V 矩阵以块的形式从 HBM 中加载到 SRAM 中，计算 O=PV ，将 O 写入到 HBM 中。</li></ul><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>self-attention 算子涉及到的和 HBM 数据传输过程如上图所示，很明显需要从 HBM 中读取 5 次，写入 HBM 3 次，HBM 访存量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>A</mi><mi>C</mi><mo>=</mo><mn>4</mn><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>3</mn><mi>N</mi><mi>d</mi></mrow><annotation encoding="application/x-tex">MAC = 4N^2 + 3Nd</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">4</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6944em;"></span><span class="mord">3</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mord mathnormal">d</span></span></span></span>，很明显标准注意力的 HBM 访问代价 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>A</mi><mi>C</mi></mrow><annotation encoding="application/x-tex">MAC</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.07153em;">C</span></span></span></span> 随序列长度增加呈二次方增长。</p><p>而 self-attention 的计算量为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><msup><mi>N</mi><mn>2</mn></msup><mi>d</mi><mo>+</mo><msup><mi>N</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">4N^2d+N^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8974em;vertical-align:-0.0833em;"></span><span class="mord">4</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8141em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span>，标准注意力算子的操作强度 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>=</mo><mfrac><mrow><mn>4</mn><msup><mi>N</mi><mn>2</mn></msup><mi>d</mi><mo>+</mo><msup><mi>N</mi><mn>2</mn></msup></mrow><mrow><mn>4</mn><msup><mi>N</mi><mn>2</mn></msup><mo>+</mo><mn>3</mn><mi>N</mi><mi>d</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">= \\frac{4N^2d+N^2}{4N^2 + 3Nd}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.3669em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.4213em;vertical-align:-0.4033em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0179em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7463em;"><span style="top:-2.786em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mbin mtight">+</span><span class="mord mtight">3</span><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="mord mathnormal mtight">d</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal mtight">d</span><span class="mbin mtight">+</span><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913em;"><span style="top:-2.931em;margin-right:0.0714em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.4033em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>。公式可看出，标准注意力算子是一个很明显的内存受限型算子。<br><img src="'+p+'" alt="" loading="lazy"></p><h3 id="_1-2-fav1整体介绍" tabindex="-1"><a class="header-anchor" href="#_1-2-fav1整体介绍"><span>1.2 FAV1整体介绍</span></a></h3><h4 id="挑战" tabindex="-1"><a class="header-anchor" href="#挑战"><span>挑战</span></a></h4><p>在Flash Attention出来之前，已经有了很多了fusedattention算子，但是仔细看，可以发现这其实不是真正的融合算子，只是把matmul kernel、scale kernel、softmax kernel的接口在一个fusedattention算子里面按照计算顺序调了一下，这种手法最多减少了pytorch、TF等框架对算子的调度开销，其实不能真正解决对HBM或者显存的memory traffic。</p><p><strong>融合MHA的挑战在于两点：</strong></p><p>1.解决softmax，因为softmax是一个row-wise（以行为单位）的操作，必须要遍历softmax一行才能得到结果，由此，后面的matmul不得不等待这个过程，导致并行性降低</p><p>2.在寄存器和shared memory复用数据做计算，而不是去HBM或显存上去读数据来计算，然而寄存器数量和shared memory (也就是图中的SRAM) 大小都有限，在左图的情况下，显然无法将softmax的结果存到这两个存储单元里面供下一个matmul复用，下一个matmul不得不去HBM或显存上读数据</p><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h4 id="整体思想" tabindex="-1"><a class="header-anchor" href="#整体思想"><span>整体思想</span></a></h4><p>FAV1算法整体做到了如下的两部分：</p><ul><li>Tiling（在前向和后向传递中使用）: <ol><li><p>（解决挑战1）Online Softmax 实现在一个 for 循环中计算<br><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">d_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，<strong>FlashAttention-v1 基于它的思想更进一步，实现在一个 for 循环中计算 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>m</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">m_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5806em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 、<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">d_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和注意力输出 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>O</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">O_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3117em;"><span style="top:-2.55em;margin-left:-0.0278em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> ，也就是说，在一个 kernel 中实现 attention 的所有操作</strong>。</p></li><li><p>（解决挑战2）再<strong>通过分块 Tiling 技术</strong>，将输入的 Q、K、V 矩阵拆分为多个块，将其从较慢的 HBM 加载到更快的 SRAM 中，从而大大减少了 HBM 访问次数（内存读/写的次数），然后分别计算这些块的注意力输出，最后，将每个块的输出按正确的归一化因子缩放之后相加后可得到精确的注意力输出。</p></li></ol></li><li>重新计算（仅在后向传递中使用）:核心思想为不为反向传递存储S、P矩阵，但是输出softmax的l和m，在反向传播时，重新计算S、P矩阵。</li></ul><h3 id="_1-3-fav1算法流程" tabindex="-1"><a class="header-anchor" href="#_1-3-fav1算法流程"><span>1.3 FAV1算法流程</span></a></h3>',16)),(0,t.Q3)(' ![](Figure/flashattention/FAV1_1.png "FlashAttention Block Diagram") '),(0,t.Q3)(' <p align="center">\n  <img src="Figure/FA1.png" width="500" alt="核心思想"/>\n</p> '),a[6]||(a[6]=(0,t.Fv)('<p>Flashattention总体的算法流程图如下：<br><img src="'+i+'" alt="" loading="lazy"></p><ol><li><p><img src="'+r+'" alt="" loading="lazy"><br> 设置块的行大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mi>r</mi></msub><mo>=</mo><mfrac><mi>M</mi><mrow><mn>4</mn><mi>d</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">B_r = \\frac{M}{4d}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2173em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mord mathnormal mtight">d</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>，块的列大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mi>c</mi></msub><mo>=</mo><mi>min</mi><mo>⁡</mo><mrow><mo fence="true">(</mo><mfrac><mi>M</mi><mrow><mn>4</mn><mi>d</mi></mrow></mfrac><mo separator="true">,</mo><mi>d</mi><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">B_c = \\min\\left(\\frac{M}{4d}, d\\right)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1.2223em;vertical-align:-0.35em;"></span><span class="mop">min</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="minner"><span class="mopen delimcenter" style="top:0em;"><span class="delimsizing size1">(</span></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8723em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mord mathnormal mtight">d</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">M</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.1667em;"></span><span class="mord mathnormal">d</span><span class="mclose delimcenter" style="top:0em;"><span class="delimsizing size1">)</span></span></span></span></span></span> 。 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>min</mi><mo>⁡</mo></mrow><annotation encoding="application/x-tex">\\min</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6679em;"></span><span class="mop">min</span></span></span></span> 函数的目的是防止块大小 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>B</mi><mi>r</mi></msub><mo>×</mo><msub><mi>B</mi><mi>c</mi></msub><mo>&gt;</mo><mi>M</mi><mi mathvariant="normal">/</mi><mn>4</mn></mrow><annotation encoding="application/x-tex">B_r \\times B_c &gt; M/4</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="mord">/4</span></span></span></span>，这样就无法把 4 个这样的块放到 SRAM 里，后面我们会看到为什么是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>4</mn><mo>×</mo><msub><mi>B</mi><mi>r</mi></msub><mo>×</mo><msub><mi>B</mi><mi>c</mi></msub></mrow><annotation encoding="application/x-tex">4 \\times B_r \\times B_c</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7278em;vertical-align:-0.0833em;"></span><span class="mord">4</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">×</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05017em;">B</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">c</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 的块。</p></li><li><p><img src="'+c+'" alt="" loading="lazy"><br> 我们把结果矩阵O初始化为零，后面会逐步把中间结果累加进去，所以零是合适的初始值。类似的是l(注意：对于每一行来说，它是一个标量，用于累加指数和，由于输出有N行，所以这里的l是长度为N的向量)。m用于记录每一行当前最大的值，所以也是长度为N，而-inf是求max的合适初始值。</p></li></ol><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_1-4-fav1算法的数学证明" tabindex="-1"><a class="header-anchor" href="#_1-4-fav1算法的数学证明"><span>1.4 FAV1算法的数学证明</span></a></h3><h4 id="_1-4-1-online-softmax" tabindex="-1"><a class="header-anchor" href="#_1-4-1-online-softmax"><span>1.4.1 Online Softmax</span></a></h4><p>这个证明略，详细内容见笔记<a href="https://summer536.github.io/Notes/zh/posts/softmax.html" target="_blank" rel="noopener noreferrer">Naive -&gt; Safe -&gt; Online Softmax</a></p><h4 id="_1-4-2-算法流程" tabindex="-1"><a class="header-anchor" href="#_1-4-2-算法流程"><span>1.4.2 算法流程</span></a></h4><h4 id="_1-4-3-算法实现" tabindex="-1"><a class="header-anchor" href="#_1-4-3-算法实现"><span>1.4.3 算法实现</span></a></h4><h2 id="二、flashattention-v2" tabindex="-1"><a class="header-anchor" href="#二、flashattention-v2"><span>二、Flashattention-V2</span></a></h2><h2 id="三、flashattention-v3" tabindex="-1"><a class="header-anchor" href="#三、flashattention-v3"><span>三、Flashattention-V3</span></a></h2><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><h2 id="待更新" tabindex="-1"><a class="header-anchor" href="#待更新"><span>待更新</span></a></h2><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h2><ol><li><a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf" target="_blank" rel="noopener noreferrer">Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.</a></li><li><a href="https://arxiv.org/pdf/2307.08691" target="_blank" rel="noopener noreferrer">Dao, T., et al. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. </a></li><li><a href="https://fancyerii.github.io/2023/10/23/flashattention/" target="_blank" rel="noopener noreferrer">Flash Attention V1论文解读-李理</a></li><li><a href="https://www.armcvai.cn/2024-10-02/flashattention1-paper.html" target="_blank" rel="noopener noreferrer">Flash Attention V1论文解读-Zhang</a></li><li><a href="https://www.armcvai.cn/2024-10-05/flashattention2-paper.html" target="_blank" rel="noopener noreferrer">Flash Attention V2论文解读-Zhang</a></li><li><a href="https://www.armcvai.cn/2024-10-06/flashattention3-paper.html" target="_blank" rel="noopener noreferrer">Flash Attention V3论文解读-Zhang</a></li><li><a href="https://www.armcvai.cn/2024-10-07/flashattention1-2-3-summary.html" target="_blank" rel="noopener noreferrer">flashattention1-2-3 系列总结-Zhang</a></li><li><a href="https://mp.weixin.qq.com/s?__biz=Mzg2ODk4MzE2MQ==&amp;mid=2247483875&amp;idx=1&amp;sn=a23ef737b03e5bdec1892a8818de0704&amp;chksm=cea549f5f9d2c0e3832f10031de98dd4a243fd2411ffdacb434cc64a473452a148fc149ded47&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">Flash Attention1-真正意义上的scale dot product attention的算子融合-AI不止算法</a></li><li><a href="https://www.bilibili.com/video/BV1gzBqY4Evw?spm_id_from=333.788.videopod.sections&amp;vd_source=f058beebb64c488b55915da416ee6086" target="_blank" rel="noopener noreferrer">FlashAttentionV1V2算法解释-AI不止算法Bilibili</a></li></ol>',14))])}]]),d=JSON.parse('{"path":"/zh/posts/flashattention.html","title":"Flashattention","lang":"zh-CN","frontmatter":{"title":"Flashattention","date":"2025-05-21T00:00:00.000Z","readingTime":600,"category":["机器学习","深度学习","笔记"],"tag":["注意力机制","GPU优化","大模型","算法优化"],"isOriginal":true,"description":"简介 Flashattention是一种高效的注意力机制计算方法，由斯坦福大学研究团队在2022年提出。它通过优化GPU内存访问模式，显著提高了Transformer模型中注意力计算的速度并降低了内存使用。作为大模型训练和推理的关键优化技术，Flashattention已被广泛应用于各种大型语言模型中。","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Flashattention\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-05-21T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-20T14:29:40.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/Summer536\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/posts/flashattention.html"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"Flashattention"}],["meta",{"property":"og:description","content":"简介 Flashattention是一种高效的注意力机制计算方法，由斯坦福大学研究团队在2022年提出。它通过优化GPU内存访问模式，显著提高了Transformer模型中注意力计算的速度并降低了内存使用。作为大模型训练和推理的关键优化技术，Flashattention已被广泛应用于各种大型语言模型中。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-20T14:29:40.000Z"}],["meta",{"property":"article:tag","content":"算法优化"}],["meta",{"property":"article:tag","content":"大模型"}],["meta",{"property":"article:tag","content":"GPU优化"}],["meta",{"property":"article:tag","content":"注意力机制"}],["meta",{"property":"article:published_time","content":"2025-05-21T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-20T14:29:40.000Z"}]]},"git":{"createdTime":1747123438000,"updatedTime":1747751380000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":15,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":5.03,"words":1509},"filePathRelative":"zh/posts/flashattention.md","excerpt":"\\n<h2>简介</h2>\\n<p>Flashattention是一种高效的注意力机制计算方法，由斯坦福大学研究团队在2022年提出。它通过优化GPU内存访问模式，显著提高了Transformer模型中注意力计算的速度并降低了内存使用。作为大模型训练和推理的关键优化技术，Flashattention已被广泛应用于各种大型语言模型中。</p>\\n","autoDesc":true}')}}]);