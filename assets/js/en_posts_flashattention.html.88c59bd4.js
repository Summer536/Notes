"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[3874],{6262:(e,t)=>{t.A=(e,t)=>{const i=e.__vccOpts||e;for(const[e,a]of t)i[e]=a;return i}},8329:(e,t,i)=>{i.r(t),i.d(t,{comp:()=>s,data:()=>o});var a=i(641);const n={},s=(0,i(6262).A)(n,[["render",function(e,t){return(0,a.uX)(),(0,a.CE)("div",null,[t[0]||(t[0]=(0,a.Lk)("h1",{id:"flashattention-technical-details",tabindex:"-1"},[(0,a.Lk)("a",{class:"header-anchor",href:"#flashattention-technical-details"},[(0,a.Lk)("span",null,"Flashattention Technical Details")])],-1)),t[1]||(t[1]=(0,a.Lk)("h2",{id:"introduction",tabindex:"-1"},[(0,a.Lk)("a",{class:"header-anchor",href:"#introduction"},[(0,a.Lk)("span",null,"Introduction")])],-1)),t[2]||(t[2]=(0,a.Lk)("p",null,"Flashattention is an efficient attention mechanism calculation method proposed by a Stanford University research team in 2022. It significantly improves the speed of attention calculations in Transformer models and reduces memory usage by optimizing GPU memory access patterns. As a key optimization technology for large model training and inference, Flashattention has been widely applied in various large language models.",-1)),(0,a.Q3)(" more "),t[3]||(t[3]=(0,a.Fv)('<h2 id="core-concept" tabindex="-1"><a class="header-anchor" href="#core-concept"><span>Core Concept</span></a></h2><p>Main bottlenecks of traditional attention mechanisms:</p><ul><li>Need to store the complete attention matrix, leading to quadratic growth in memory usage</li><li>Large amount of memory read and write operations resulting in low GPU computational efficiency</li></ul><p>Flashattention innovations:</p><ul><li>Using tiling computation strategy</li><li>Fully utilizing GPU&#39;s SRAM (fast on-chip memory)</li><li>Reducing access to HBM (high bandwidth memory)</li></ul><h2 id="technical-principles" tabindex="-1"><a class="header-anchor" href="#technical-principles"><span>Technical Principles</span></a></h2><h3 id="algorithm-flow" tabindex="-1"><a class="header-anchor" href="#algorithm-flow"><span>Algorithm Flow</span></a></h3><ol><li>Dividing the input sequence into multiple small blocks</li><li>Loading only a small block of data into SRAM at a time</li><li>Computing local attention in SRAM</li><li>Combining local results to obtain global attention based on mathematical equivalence</li></ol><h3 id="performance-improvements" tabindex="-1"><a class="header-anchor" href="#performance-improvements"><span>Performance Improvements</span></a></h3><ul><li>Computation speed: 2-4 times faster than traditional implementations</li><li>Memory usage: Significantly reduced memory consumption, supporting longer sequences</li><li>Training efficiency: Accelerating the training process of large models</li></ul><h2 id="flashattention-2" tabindex="-1"><a class="header-anchor" href="#flashattention-2"><span>Flashattention-2</span></a></h2><p>Based on the original Flashattention, the research team further proposed Flashattention-2, bringing more improvements:</p><ul><li>Optimized tiling strategy</li><li>Improved I/O complexity</li><li>Specialized optimization for different GPU architectures</li></ul><h2 id="application-scenarios" tabindex="-1"><a class="header-anchor" href="#application-scenarios"><span>Application Scenarios</span></a></h2><p>Flashattention has been widely applied in:</p><ul><li>Large language models (such as the GPT series)</li><li>Long sequence processing models</li><li>Vision Transformer models</li></ul><h2 id="implementation-and-usage" tabindex="-1"><a class="header-anchor" href="#implementation-and-usage"><span>Implementation and Usage</span></a></h2><p>Main implementation libraries:</p><ul><li>xFormers</li><li>Flash-Attention (GitHub)</li><li>PyTorch nightly version has integrated it</li></ul><p>Simple usage example:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Simplified example using FlashAttention-2</span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> flash_attn </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> flash_attn_func</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># Assume q, k, v are query, key, value matrices</span></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># [batch_size, seq_len, num_heads, head_dim]</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">output </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> flash_attn_func</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(q, k, v, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">causal</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="future-development" tabindex="-1"><a class="header-anchor" href="#future-development"><span>Future Development</span></a></h2><p>Attention mechanism optimization is still rapidly developing:</p><ul><li>Supporting longer context windows</li><li>Reducing memory requirements</li><li>Improving throughput and inference speed</li></ul><h2 id="references" tabindex="-1"><a class="header-anchor" href="#references"><span>References</span></a></h2><ol><li>Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.</li><li>Dao, T., et al. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.</li></ol>',26))])}]]),o=JSON.parse('{"path":"/en/posts/flashattention.html","title":"Flashattention","lang":"en-US","frontmatter":{"title":"Flashattention","date":"2025-05-10T00:00:00.000Z","category":["Machine Learning","Deep Learning","Notes"],"tag":["Attention Mechanism","GPU Optimization","Large Models","Algorithm Optimization"],"cover":"/assets/images/cover3.jpg","isOriginal":true,"description":"Introduction Flashattention is an efficient attention mechanism calculation method proposed by a Stanford University research team in 2022. It significantly improves the speed o...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Flashattention\\",\\"image\\":[\\"https://your-domain.com/Notes/assets/images/cover3.jpg\\"],\\"datePublished\\":\\"2025-05-10T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-13T09:09:51.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/your-username\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/en/posts/flashattention.html"}],["meta",{"property":"og:site_name","content":"GYQ\'s Blog"}],["meta",{"property":"og:title","content":"Flashattention"}],["meta",{"property":"og:description","content":"Introduction Flashattention is an efficient attention mechanism calculation method proposed by a Stanford University research team in 2022. It significantly improves the speed o..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://your-domain.com/Notes/assets/images/cover3.jpg"}],["meta",{"property":"og:locale","content":"en-US"}],["meta",{"property":"og:locale:alternate","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-13T09:09:51.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://your-domain.com/Notes/assets/images/cover3.jpg"}],["meta",{"name":"twitter:image:alt","content":"Flashattention"}],["meta",{"property":"article:tag","content":"Algorithm Optimization"}],["meta",{"property":"article:tag","content":"Large Models"}],["meta",{"property":"article:tag","content":"GPU Optimization"}],["meta",{"property":"article:tag","content":"Attention Mechanism"}],["meta",{"property":"article:published_time","content":"2025-05-10T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-13T09:09:51.000Z"}],["link",{"rel":"alternate","hreflang":"zh-cn","href":"https://your-domain.com/Notes/posts/flashattention.html"}]]},"git":{"createdTime":1747127391000,"updatedTime":1747127391000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":1,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":1.26,"words":377},"filePathRelative":"en/posts/flashattention.md","excerpt":"\\n<h2>Introduction</h2>\\n<p>Flashattention is an efficient attention mechanism calculation method proposed by a Stanford University research team in 2022. It significantly improves the speed of attention calculations in Transformer models and reduces memory usage by optimizing GPU memory access patterns. As a key optimization technology for large model training and inference, Flashattention has been widely applied in various large language models.</p>\\n","autoDesc":true}')}}]);