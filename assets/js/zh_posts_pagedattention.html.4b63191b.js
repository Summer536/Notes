"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[7812],{336:(t,e,a)=>{a.r(e),a.d(e,{comp:()=>r,data:()=>i});var n=a(641);const o={},r=(0,a(6262).A)(o,[["render",function(t,e){return(0,n.uX)(),(0,n.CE)("div",null,[e[0]||(e[0]=(0,n.Lk)("h1",{id:"flashattention",tabindex:"-1"},[(0,n.Lk)("a",{class:"header-anchor",href:"#flashattention"},[(0,n.Lk)("span",null,"Flashattention")])],-1)),e[1]||(e[1]=(0,n.Lk)("h2",{id:"简介",tabindex:"-1"},[(0,n.Lk)("a",{class:"header-anchor",href:"#简介"},[(0,n.Lk)("span",null,"简介")])],-1)),e[2]||(e[2]=(0,n.Lk)("p",null,"PagedAttention是vLLM中提出的一个用于加速Transformer模型推理的注意力机制计算方法。",-1)),(0,n.Q3)(" more ")])}]]),i=JSON.parse('{"path":"/zh/posts/pagedattention.html","title":"PagedAttention","lang":"zh-CN","frontmatter":{"title":"PagedAttention","date":"2025-05-23T00:00:00.000Z","readingTime":600,"category":["笔记"],"tag":["GPU优化","大模型","算法优化"],"isOriginal":true,"description":"简介 PagedAttention是vLLM中提出的一个用于加速Transformer模型推理的注意力机制计算方法。","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"PagedAttention\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-05-23T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-21T13:05:48.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/Summer536\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/posts/pagedattention.html"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"PagedAttention"}],["meta",{"property":"og:description","content":"简介 PagedAttention是vLLM中提出的一个用于加速Transformer模型推理的注意力机制计算方法。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-21T13:05:48.000Z"}],["meta",{"property":"article:tag","content":"算法优化"}],["meta",{"property":"article:tag","content":"大模型"}],["meta",{"property":"article:tag","content":"GPU优化"}],["meta",{"property":"article:published_time","content":"2025-05-23T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-21T13:05:48.000Z"}]]},"git":{"createdTime":1747832748000,"updatedTime":1747832748000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":1,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":0.19,"words":58},"filePathRelative":"zh/posts/pagedattention.md","excerpt":"\\n<h2>简介</h2>\\n<p>PagedAttention是vLLM中提出的一个用于加速Transformer模型推理的注意力机制计算方法。</p>\\n","autoDesc":true}')},6262:(t,e)=>{e.A=(t,e)=>{const a=t.__vccOpts||t;for(const[t,n]of e)a[t]=n;return a}}}]);