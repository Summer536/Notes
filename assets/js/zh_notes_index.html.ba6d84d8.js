"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[43],{6262:(e,t)=>{t.A=(e,t)=>{const a=e.__vccOpts||e;for(const[e,n]of t)a[e]=n;return a}},8833:(e,t,a)=>{a.r(t),a.d(t,{comp:()=>o,data:()=>i});var n=a(641);const r={},o=(0,a(6262).A)(r,[["render",function(e,t){return(0,n.uX)(),(0,n.CE)("div",null,t[0]||(t[0]=[(0,n.Fv)('<h1 id="学习笔记" tabindex="-1"><a class="header-anchor" href="#学习笔记"><span>学习笔记</span></a></h1><p>这里收集了我的技术学习笔记和心得体会，包括各种技术栈的学习记录。</p><h2 id="笔记列表" tabindex="-1"><a class="header-anchor" href="#笔记列表"><span>笔记列表</span></a></h2><ul><li><a href="/zh/posts/flashattention.html" target="_blank" rel="noopener noreferrer">Flashattention技术详解</a> - 高效注意力机制计算方法的详细解析</li><li><a href="/zh/posts/cuda-tech-stack.html" target="_blank" rel="noopener noreferrer">CUDA技术栈</a> - CUDA并行计算平台及编程模型介绍</li></ul>',4)]))}]]),i=JSON.parse('{"path":"/zh/notes/","title":"学习笔记","lang":"zh-CN","frontmatter":{"title":"学习笔记","icon":"folder-open","description":"学习笔记 这里收集了我的技术学习笔记和心得体会，包括各种技术栈的学习记录。 笔记列表 Flashattention技术详解 - 高效注意力机制计算方法的详细解析 CUDA技术栈 - CUDA并行计算平台及编程模型介绍","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"学习笔记\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-05-13T08:43:02.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/your-username\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/notes/"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"学习笔记"}],["meta",{"property":"og:description","content":"学习笔记 这里收集了我的技术学习笔记和心得体会，包括各种技术栈的学习记录。 笔记列表 Flashattention技术详解 - 高效注意力机制计算方法的详细解析 CUDA技术栈 - CUDA并行计算平台及编程模型介绍"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-13T08:43:02.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-13T08:43:02.000Z"}]]},"git":{"createdTime":1747125782000,"updatedTime":1747125782000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":1,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":0.3,"words":89},"filePathRelative":"zh/notes/README.md","excerpt":"\\n<p>这里收集了我的技术学习笔记和心得体会，包括各种技术栈的学习记录。</p>\\n<h2>笔记列表</h2>\\n<ul>\\n<li><a href=\\"/zh/posts/flashattention.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Flashattention技术详解</a> - 高效注意力机制计算方法的详细解析</li>\\n<li><a href=\\"/zh/posts/cuda-tech-stack.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">CUDA技术栈</a> - CUDA并行计算平台及编程模型介绍</li>\\n</ul>","autoDesc":true}')}}]);