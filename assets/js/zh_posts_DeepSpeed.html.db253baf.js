"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[1724],{3993:(a,s,t)=>{t.r(s),t.d(s,{comp:()=>y,data:()=>x});var n=t(641);const e=t.p+"assets/img/cunchu.aa9b7430.jpg",p=t.p+"assets/img/mix.93233601.png",i=t.p+"assets/img/memory.79e2c273.jpg",l=t.p+"assets/img/zero.5c91f645.jpg",r=t.p+"assets/img/zero1.b03010d4.jpg",m=t.p+"assets/img/zero1_2.3f389449.jpg",o=t.p+"assets/img/zero1_3.88f2182b.jpg",c=t.p+"assets/img/zero2_1.f9322cf4.jpg",h=t.p+"assets/img/zero2_2.ed9afae1.jpg",g=t.p+"assets/img/zero2_3.c050a5c7.jpg",d=t.p+"assets/img/zero3_1.defd4371.jpg",u=t.p+"assets/img/zero3_2.5cefb97d.jpg",f=t.p+"assets/img/zero_offload.3fae91fb.jpg",w={},y=(0,t(6262).A)(w,[["render",function(a,s){return(0,n.uX)(),(0,n.CE)("div",null,[s[0]||(s[0]=(0,n.Lk)("h1",{id:"deepspeed",tabindex:"-1"},[(0,n.Lk)("a",{class:"header-anchor",href:"#deepspeed"},[(0,n.Lk)("span",null,"DeepSpeed")])],-1)),s[1]||(s[1]=(0,n.Lk)("h2",{id:"简介",tabindex:"-1"},[(0,n.Lk)("a",{class:"header-anchor",href:"#简介"},[(0,n.Lk)("span",null,"简介")])],-1)),s[2]||(s[2]=(0,n.Lk)("p",null,"本文将从大模型训练过程中GPU的存储内容谈起，分析分布式数据并行DDP的显存占用问题，进而介绍解决方法-DeepSpeed框架及其三种ZeRO策略。",-1)),(0,n.Q3)(" more "),s[3]||(s[3]=(0,n.Fv)('<h2 id="引言" tabindex="-1"><a class="header-anchor" href="#引言"><span>引言</span></a></h2><p>数据并行DP有两个显著问题：</p><ul><li>存储开销大。每块GPU上都存了一份完整的模型，造成冗余。</li><li>通讯开销大。Server需要和每一个Worker进行梯度传输。当Server和Worker不在一台机器上时，Server的带宽将会成为整个系统的计算效率瓶颈。</li></ul><p>DDP通过Ring AllReduce将通讯压力均衡到每个Worker上，解决了前者问题。但显存占用问题依然存在，由微软开发的ZeRO（零冗余优化），它是DeepSpeed这一分布式训练框架的核心，被用来解决大模型训练中的显存开销问题。<strong>ZeRO的思想就是用通讯换显存</strong>。</p><h2 id="一、大模型训练过程中gpu的存储" tabindex="-1"><a class="header-anchor" href="#一、大模型训练过程中gpu的存储"><span>一、大模型训练过程中GPU的存储</span></a></h2><h3 id="_1-1-存储分类" tabindex="-1"><a class="header-anchor" href="#_1-1-存储分类"><span>1.1 存储分类</span></a></h3><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>存储主要分为两大块：Model States和Residual States。</p><p><strong>Model States</strong>指和模型本身息息相关的，<strong>必须存储</strong>的内容，具体包括：</p><ul><li>optimizer states：Adam优化算法中的momentum(一阶动量)和variance方差(二阶动量)</li><li>gradients：模型梯度</li><li>parameters：模型参数W</li></ul><p><strong>Residual States指并非模型必须的</strong>，但在训练过程中会额外产生的内容，具体包括：</p><ul><li>activation：激活值。在流水线并行中我们曾详细介绍过。在backward过程中使用链式法则计算梯度时会用到。有了它算梯度会更快，但它不是必须存储的，因为可以通过重新做Forward来算它。</li><li>temporary buffers: 临时存储。例如把梯度发送到某块GPU上做加总聚合时产生的存储。</li><li>unusable fragment memory：碎片化的存储空间。虽然总存储空间是够的，但是如果取不到连续的存储空间，相关的请求也会被fail掉。对这类空间浪费可以通过内存整理来解决。</li></ul><h3 id="_1-2-混合精度训练" tabindex="-1"><a class="header-anchor" href="#_1-2-混合精度训练"><span>1.2 混合精度训练</span></a></h3><p>知道了存储分类，进一步，我们想知道，假设模型的参数W大小是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Φ</span></span></span></span>，那么每一类存储具体占了多大的空间呢？</p><p>在分析这个问题前，我们需要来了解<strong>混合精度训练</strong><br> 对于模型，我们肯定希望其参数越精准越好，也即我们用fp32（单精度浮点数，存储占4byte）来表示参数W。但是在forward和backward的过程中，fp32的计算开销也是庞大的。那么能否在计算的过程中，引入fp16或bf16（半精度浮点数，存储占2byte），来减轻计算压力呢？于是，混合精度训练就产生了，它的步骤如下图：</p><figure><img src="'+p+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><ol><li><strong>存储一份fp32的parameter，momentum和variance（统称model states）</strong></li><li>在forward开始之前，额外开辟一块存储空间，将fp32 parameter减半到fp16 parameter。</li><li>正常做forward和backward，在此之间产生的activation和gradients，都用fp16进行存储。</li><li><strong>用fp16 gradients去更新fp32下的model states</strong>，<strong>用fp16 parameter去更新fp32下的model states</strong>。</li><li>当模型收敛后，fp32的parameter就是最终的参数输出。</li></ol><p>通过这种方式，混合精度训练在计算开销和模型精度上做了权衡。</p><h3 id="_1-3-存储空间计算" tabindex="-1"><a class="header-anchor" href="#_1-3-存储空间计算"><span>1.3 存储空间计算</span></a></h3><p>我们可以来计算模型在训练时需要的存储大小了，假设模型的参数W大小是<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Φ</span></span></span></span>，以byte为单位，存储如下：</p><figure><img src="'+i+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>这里<strong>暂不将activation纳入统计范围</strong>，原因是：</p><ul><li>activation不仅与模型参数相关，还与batch size相关</li><li>activation的存储不是必须的。存储activation只是为了在用链式法则做backward的过程中，计算梯度更快一些。但你永远可以通过只保留最初的输入X，重新做forward来得到每一层的activation（虽然实际中并不会这么极端）。</li><li>因为activation的这种灵活性，纳入它后不方便衡量系统性能随模型增大的真实变动情况。因此在这里不考虑它。</li></ul><h2 id="二、deepspeed-zero-zero-redundancy-optimizer" tabindex="-1"><a class="header-anchor" href="#二、deepspeed-zero-zero-redundancy-optimizer"><span>二、DeepSpeed-ZeRO（Zero Redundancy Optimizer）</span></a></h2><p>零冗余优化器（Zero Redundancy Optimizer），是DeepSpeed-ZeRO的核心，它通过将优化器状态、梯度和模型参数进行分割，从而减少显存占用。</p><p>在整个训练中，有很多states并不会每时每刻都用到，举例来说；</p><ul><li>Adam优化下的optimizer states只在最终做update时才用到(可对优化器os进行优化)</li><li>数据并行中，gradients只在最后做AllReduce和updates时才用到(可对梯度g进行优化)</li><li>参数W只在做forward和backward的那一刻才用到(可对参数W进行优化)</li></ul><p>诸如此类。</p><p>所以，ZeRO想了一个简单粗暴的办法：<strong>如果数据算完即废，等需要的时候，我再想办法从个什么地方拿回来，那不就省了一笔存储空间吗？</strong></p><p>沿着这个思路，我们逐一来看ZeRO是如何递进做存储优化的。</p><ul><li>ZeRO Stage 1：仅对优化器状态进行分割，每个GPU中仍有完整的模型参数和梯度数据</li><li>ZeRO Stage 2：对优化器状态和梯度进行分割</li><li>ZeRO Stage 3：对优化器状态、梯度和模型参数全部进行分割</li></ul><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h2 id="三、zero-dp" tabindex="-1"><a class="header-anchor" href="#三、zero-dp"><span>三、ZeRO-DP</span></a></h2><p><em>优化器状态（O, fp32），参数（W, fp16）和梯度（G, fp16）</em></p><h3 id="_3-1-zero-stage1-优化状态分割" tabindex="-1"><a class="header-anchor" href="#_3-1-zero-stage1-优化状态分割"><span>3.1 ZeRO-Stage1（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：优化状态分割）</span></a></h3><p>ZeRO-Stage1的思路是：<strong>将optimizer states进行分割</strong>。每块GPU上各自维护一份。这样就减少了相当一部分的显存开销。如下图：</p><figure><img src="'+r+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>此时，整体数据并行的流程如下：</p><p>（1）每块GPU上存一份完整的参数W。将一个batch的数据分成3份，每块GPU各吃一份，做完一轮foward和backward后，各得一份梯度。</p><p>（2）对<strong>梯度做一次AllReduce(N分片-&gt;1规约计算-&gt;N广播)，得到完整的梯度G，产生单卡通讯量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">2\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">2Φ</span></span></span></span></strong>。</p><p>（3）得到完整梯度G，就可以对W做更新。我们知道W的更新由optimizer states和梯度共同决定。由于每块GPU上只保管部分optimizer states，因此只能将相应的W（蓝色部分）进行更新。</p><figure><img src="'+m+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>（4）此时，每块GPU上都有部分W没有完成更新（图中白色部分）。所以我们需要<strong>对W做一次All-Gather(N分片-&gt;1合并-&gt;N广播)</strong>，从别的GPU上把更新好的部分W取回来。产生<strong>单卡通讯量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Φ</span></span></span></span></strong>。</p><p>（5）最后，每块GPU上都有完整的W，可以进行下一轮的训练。</p><p>做完<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{os}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>后，设GPU个数为<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>N</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">N_d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>，显存和通讯量的情况如下：</p><figure><img src="'+o+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>增加1.5倍单卡通讯开销的基础上，将单卡存储降低了4倍</em></p><h3 id="_3-2-zero-stage2-优化状态和梯度分割" tabindex="-1"><a class="header-anchor" href="#_3-2-zero-stage2-优化状态和梯度分割"><span>3.2 ZeRO-Stage2（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi></mrow></msub><mo>+</mo><msub><mi>P</mi><mi>g</mi></msub></mrow><annotation encoding="application/x-tex">P_{os}+P_{g}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span></span></span></span>：优化状态和梯度分割）</span></a></h3><p>更近一步，我们把梯度也拆开，每个GPU格子维护一块梯度。<br><img src="'+c+'" alt="" loading="lazy"><br> 此时，数据并行的整体流程如下：</p><p>（1）每块GPU上存一份完整的参数W。将一个batch的数据分成3份，每块GPU各吃一份，做完一轮foward和backward后，算得一份完整的梯度（下图中绿色+白色）。</p><p>（2）对<strong>梯度做一次Reduce-Scatter</strong>，保证每个GPU上所维持的那块梯度是聚合梯度。例如对GPU1，它负责维护G1，因此其他的GPU只需要把G1对应位置的梯度发给GPU1做加总就可。汇总完毕后，白色块对GPU无用，可以从显存中移除。<strong>单卡通讯量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Φ</span></span></span></span></strong>。</p><figure><img src="'+h+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>（3）每块GPU用自己对应的O和G去更新相应的W。更新完毕后，每块GPU维持了一块更新完毕的W。同理，<strong>对W做一次All-Gather</strong>，将别的GPU算好的W同步到自己这来。<strong>单卡通讯量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Φ</span></span></span></span></strong>。</p><p>再次比对下显存和通讯量：</p><figure><img src="'+g+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>和朴素DP相比，存储降了8倍，单卡通讯量持平</em></p><h3 id="_3-3-zero-stage3-优化状态、梯度和参数分割" tabindex="-1"><a class="header-anchor" href="#_3-3-zero-stage3-优化状态、梯度和参数分割"><span>3.3 ZeRO-Stage3（<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>o</mi><mi>s</mi></mrow></msub><mo>+</mo><msub><mi>P</mi><mi>g</mi></msub><mo>+</mo><msub><mi>P</mi><mi>w</mi></msub></mrow><annotation encoding="application/x-tex">P_{os}+P_{g}+P_{w}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">os</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.9694em;vertical-align:-0.2861em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">g</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2861em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02691em;">w</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：优化状态、梯度和参数分割）</span></a></h3><p>ZeRO的思想就是：<strong>万物皆可切，万物皆可抛</strong>。所以现在，我们把参数也切开。每块GPU置维持对应的optimizer states，gradients和parameters（即W）。<br><img src="'+d+'" alt="" loading="lazy"></p><p>数据并行的流程如下：</p><p>（1）每块GPU上只保存部分参数W。将一个batch的数据分成3份，每块GPU各吃一份。</p><p>（2）做forward时，<strong>对W做一次All-Gather</strong>，取回分布在别的GPU上的W，得到一份完整的W，<strong>单卡通讯量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Φ</span></span></span></span></strong>。forward做完，立刻把不是自己维护的W抛弃。</p><p>（3）做backward时，<strong>对W做一次All-Gather</strong>，取回完整的W，<strong>单卡通讯量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Φ</span></span></span></span></strong>。backward做完，立刻把不是自己维护的W抛弃。</p><p>（4）做完backward，算得一份完整的梯度G，<strong>对G做一次Reduce-Scatter</strong>，从别的GPU上聚合自己维护的那部分梯度，<strong>单卡通讯量<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Φ</mi></mrow><annotation encoding="application/x-tex">\\Phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Φ</span></span></span></span></strong>。聚合操作结束后，立刻把不是自己维护的G抛弃。</p><p>（5）用自己维护的O和G，更新W。由于只维护部分W，因此无需再对W做任何AllReduce操作。</p><p>显存和通讯量如下：</p><figure><img src="'+u+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p><em>我们用1.5倍的通讯开销，换回近120倍的显存！</em></p><p>特别值得注意的是，最新的Zero3采用的层内（intra）分割，也就是把完整的权重矩阵分成几块，而早期的Zero3采用的是层间（inter）分割</p><p>以上两种分割方式对应了不同的通信方式：</p><ul><li>层间（inter）分割：在前向传播和反向传播时做broadcast，计算到块时，GPU0将广播到其他设备。</li><li>层内（intra）分割：在前向传播和反向传播时做all-gather，取回每一块的完整的权重，以便做梯度计算。</li></ul><h3 id="_3-4-zero-vs-模型并行" tabindex="-1"><a class="header-anchor" href="#_3-4-zero-vs-模型并行"><span>3.4 ZeRO VS 模型并行</span></a></h3><p>既然ZeRO都把参数W给切了，那它应该是个模型并行呀？为什么要归到数据并行里呢？</p><p>其实<strong>ZeRO是模型并行的形式，数据并行的实质</strong>。<strong>模型并行</strong>，是指在forward和backward的过程中，我只需要用<strong>自己维护的那块W来计算</strong>就行。即同样的输入X，每块GPU上各算模型的一部分，最后通过某些方式聚合结果。但<strong>对ZeRO来说</strong>，它做forward和backward的时候，是需要把各GPU上维护的W聚合起来的，即<strong>本质上还是用完整的W进行计算</strong>。它是不同的输入X，完整的参数W，最终再做聚合。</p><h2 id="四、zero-r" tabindex="-1"><a class="header-anchor" href="#四、zero-r"><span>四、ZeRO-R</span></a></h2><p>上述是对model states的显存优化，现在来看对residual states的优化。</p><h3 id="_4-1-partitioned-activation-checkpointing" tabindex="-1"><a class="header-anchor" href="#_4-1-partitioned-activation-checkpointing"><span>4.1 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>P</mi><mrow><mi>a</mi><mi>c</mi></mrow></msub></mrow><annotation encoding="application/x-tex">P_{ac}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em;">P</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.1514em;"><span style="top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">c</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：Partitioned Activation Checkpointing</span></a></h3><p>前面说过，对activation的存储是灵活的。不像optimizer states，gradients和parameters对模型更新是必须的，activation只是起到加速梯度计算的作用。因此，在哪几层保存activation，保存哪些activation都是可以灵活设置的。同样，我们也可以仿照以上切割方式，每块GPU上只维护部分的activation，需要时再从别的地方聚合过来就行。需要注意的是，activation对显存的占用一般会远高于模型本身，通讯量也是巨大的，所以这块要灵活、有效地实验设计。</p><p><strong>一句话总结，这个需要根据模型结构和计算图来做实验设计。</strong></p><h3 id="_4-2-constant-size-buffer" tabindex="-1"><a class="header-anchor" href="#_4-2-constant-size-buffer"><span>4.2 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">C_{B}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em;">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.0715em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：Constant Size Buffer</span></a></h3><p>固定大小的内存buffer，它的目的在于：</p><ul><li><strong>提升带宽利用率</strong>。当GPU数量上升，GPU间的通讯次数也上升，每次的通讯量可能下降（但总通讯量不会变）。数据切片小了，就不能很好利用带宽了。所以这个buffer起到了积攒数据的作用：等数据积攒到一定大小，再进行通讯。</li><li><strong>使得存储大小可控</strong>。在每次通讯前，积攒的存储大小是常量，是已知可控的。更方便使用者对训练中的存储消耗和通讯时间进行预估。</li></ul><h3 id="_4-3-memory-defragmentation" tabindex="-1"><a class="header-anchor" href="#_4-3-memory-defragmentation"><span>4.3 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>M</mi><mi>D</mi></msub></mrow><annotation encoding="application/x-tex">M_{D}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">M</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3283em;"><span style="top:-2.55em;margin-left:-0.109em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">D</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>：Memory Defragmentation</span></a></h3><p>在前文提过，设置机制，对碎片化的存储空间进行重新整合，整出连续的存储空间。防止出现总存储足够，但连续存储不够而引起的存储请求fail。</p><h2 id="五、zero-offload与zero-infinity" tabindex="-1"><a class="header-anchor" href="#五、zero-offload与zero-infinity"><span>五、ZeRO-Offload与ZeRO-Infinity</span></a></h2><h3 id="_5-1-zero-offload" tabindex="-1"><a class="header-anchor" href="#_5-1-zero-offload"><span>5.1 ZeRO-Offload</span></a></h3><p>最后，简单介绍一下ZeRO-Offload。它的核心思想是：<strong>显存不够，内存来凑</strong>。如果我把要存储的大头卸载(offload)到CPU上，而把计算部分放到GPU上，这样比起跨机，是不是能既降显存，也能减少一些通讯压力呢？<br> ZeRO-Offload的做法是：</p><ul><li><strong>forward和backward计算量高</strong>，因此和它们相关的部分，例如参数W（fp16），activation，就全<strong>放入GPU</strong>。</li><li><strong>update的部分计算量低</strong>，因此和它相关的部分，全部<strong>放入CPU中</strong>。例如优化器中的W(fp32)，优化器中的一阶二阶动量（fp32）和gradients(fp16)等。</li></ul><figure><img src="'+f+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><h3 id="_5-2-zero-infinity" tabindex="-1"><a class="header-anchor" href="#_5-2-zero-infinity"><span>5.2 ZeRO-Infinity</span></a></h3><p>ZeRO-infinity也是同理，它们在解决的事情都是：找个除GPU之外的地方，存数据。</p><h2 id="六、其余的优化" tabindex="-1"><a class="header-anchor" href="#六、其余的优化"><span>六、其余的优化</span></a></h2><ul><li>ZeRO ++ ：针对ZeRO3通信进行优化，通过权重量化、权重分层存储、梯度量化降低跨节点通信量</li><li>DeepSpeed Ulysses：针对长序列训练，将各个样本在序列维度上分割给参与的GPU</li></ul><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2><ol><li><p><a href="https://mp.weixin.qq.com/s/kYeNjMsesfKfoZtJPRkciA" target="_blank" rel="noopener noreferrer">大模型的分布式训练框架：deepspeed</a></p></li><li><p><a href="https://www.armcvai.cn/2023-07-03/deepspeed-optimize.html" target="_blank" rel="noopener noreferrer">deepspeed-通过系统优化和压缩加速大规模模型推理和训练</a></p></li><li><p><a href="https://arxiv.org/pdf/1910.02054" target="_blank" rel="noopener noreferrer">ZeRO: Memory Optimizations Toward Training Trillion Parameter Models</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/618865052" target="_blank" rel="noopener noreferrer">数据并行下篇( DeepSpeed ZeRO，零冗余优化)</a></p></li></ol>',94))])}]]),x=JSON.parse('{"path":"/zh/posts/DeepSpeed.html","title":"DeepSpeed","lang":"zh-CN","frontmatter":{"title":"DeepSpeed","date":"2025-07-15T00:00:00.000Z","readingTime":300,"category":["笔记"],"tag":["GPU优化","算法优化"],"isOriginal":true,"description":"简介 本文将从大模型训练过程中GPU的存储内容谈起，分析分布式数据并行DDP的显存占用问题，进而介绍解决方法-DeepSpeed框架及其三种ZeRO策略。","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"DeepSpeed\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-07-15T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-15T13:00:20.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/Summer536\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/posts/DeepSpeed.html"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"DeepSpeed"}],["meta",{"property":"og:description","content":"简介 本文将从大模型训练过程中GPU的存储内容谈起，分析分布式数据并行DDP的显存占用问题，进而介绍解决方法-DeepSpeed框架及其三种ZeRO策略。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-15T13:00:20.000Z"}],["meta",{"property":"article:tag","content":"算法优化"}],["meta",{"property":"article:tag","content":"GPU优化"}],["meta",{"property":"article:published_time","content":"2025-07-15T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-15T13:00:20.000Z"}]]},"git":{"createdTime":1750855619000,"updatedTime":1752584420000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":4,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":11.52,"words":3455},"filePathRelative":"zh/posts/DeepSpeed.md","excerpt":"\\n<h2>简介</h2>\\n<p>本文将从大模型训练过程中GPU的存储内容谈起，分析分布式数据并行DDP的显存占用问题，进而介绍解决方法-DeepSpeed框架及其三种ZeRO策略。</p>\\n","autoDesc":true}')},6262:(a,s)=>{s.A=(a,s)=>{const t=a.__vccOpts||a;for(const[a,n]of s)t[a]=n;return t}}}]);