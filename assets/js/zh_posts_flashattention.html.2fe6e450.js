"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[609],{469:(i,a,t)=>{t.r(a),t.d(a,{comp:()=>n,data:()=>l});var s=t(641);const e={},n=(0,t(6262).A)(e,[["render",function(i,a){return(0,s.uX)(),(0,s.CE)("div",null,[a[0]||(a[0]=(0,s.Lk)("h1",{id:"flashattention-技术详解",tabindex:"-1"},[(0,s.Lk)("a",{class:"header-anchor",href:"#flashattention-技术详解"},[(0,s.Lk)("span",null,"Flashattention 技术详解")])],-1)),a[1]||(a[1]=(0,s.Lk)("h2",{id:"简介",tabindex:"-1"},[(0,s.Lk)("a",{class:"header-anchor",href:"#简介"},[(0,s.Lk)("span",null,"简介")])],-1)),a[2]||(a[2]=(0,s.Lk)("p",null,"Flashattention是一种高效的注意力机制计算方法，由斯坦福大学研究团队在2022年提出。它通过优化GPU内存访问模式，显著提高了Transformer模型中注意力计算的速度并降低了内存使用。作为大模型训练和推理的关键优化技术，Flashattention已被广泛应用于各种大型语言模型中。",-1)),(0,s.Q3)(" more "),a[3]||(a[3]=(0,s.Fv)('<h2 id="核心思想" tabindex="-1"><a class="header-anchor" href="#核心思想"><span>核心思想</span></a></h2><p>传统注意力机制的主要瓶颈：</p><ul><li>需要存储完整的注意力矩阵，导致内存使用呈二次方增长</li><li>大量的内存读写操作导致GPU计算效率低下</li></ul><p>Flashattention的创新点：</p><ul><li>使用分块计算策略（tiling）</li><li>充分利用GPU的SRAM（快速片上内存）</li><li>减少对HBM（高带宽内存）的访问</li></ul><h2 id="技术原理" tabindex="-1"><a class="header-anchor" href="#技术原理"><span>技术原理</span></a></h2><h3 id="算法流程" tabindex="-1"><a class="header-anchor" href="#算法流程"><span>算法流程</span></a></h3><figure><img src="/Figure/FA1.png" alt="FlashAttention架构图" tabindex="0" loading="lazy"><figcaption>FlashAttention Block Diagram</figcaption></figure>',8)),(0,s.Q3)(' <p align="center">\n  <img src="/Figure/FA1.png" width="500" alt="核心思想"/>\n</p> '),a[4]||(a[4]=(0,s.Fv)('<p>简单使用示例:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用FlashAttention-2的简化示例</span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> flash_attn </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> flash_attn_func</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 假设q, k, v是查询、键、值矩阵</span></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># [batch_size, seq_len, num_heads, head_dim]</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">output </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> flash_attn_func</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(q, k, v, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">causal</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ol><li>将输入序列分成多个小块</li><li>每次只将一小块数据加载到SRAM中</li><li>在SRAM中计算局部注意力</li><li>根据数学等价性，合并局部结果得到全局注意力</li></ol><h3 id="性能提升" tabindex="-1"><a class="header-anchor" href="#性能提升"><span>性能提升</span></a></h3><ul><li>计算速度：比传统实现快2-4倍</li><li>内存使用：显著降低内存消耗，支持更长序列</li><li>训练效率：加速大模型训练过程</li></ul><h2 id="flashattention-2" tabindex="-1"><a class="header-anchor" href="#flashattention-2"><span>Flashattention-2</span></a></h2><p>在原始Flashattention基础上，研究团队进一步提出了Flashattention-2，带来了更多改进：</p><ul><li>优化了分块策略</li><li>改进了I/O复杂度</li><li>对不同GPU架构进行了专门优化</li></ul><h2 id="应用场景" tabindex="-1"><a class="header-anchor" href="#应用场景"><span>应用场景</span></a></h2><p>Flashattention已被广泛应用于：</p><ul><li>大型语言模型（如GPT系列）</li><li>长序列处理模型</li><li>视觉Transformer模型</li></ul><h2 id="实现和使用" tabindex="-1"><a class="header-anchor" href="#实现和使用"><span>实现和使用</span></a></h2><p>主要实现库：</p><ul><li>xFormers</li><li>Flash-Attention (GitHub)</li><li>PyTorch nightly版本已集成</li></ul><p>简单使用示例:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用FlashAttention-2的简化示例</span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> flash_attn </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> flash_attn_func</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 假设q, k, v是查询、键、值矩阵</span></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># [batch_size, seq_len, num_heads, head_dim]</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">output </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> flash_attn_func</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(q, k, v, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">causal</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="未来发展" tabindex="-1"><a class="header-anchor" href="#未来发展"><span>未来发展</span></a></h2><p>注意力机制优化仍在快速发展：</p><ul><li>支持更长的上下文窗口</li><li>降低显存需求</li><li>提高吞吐量和推理速度</li></ul><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h2><ol><li>Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.</li><li>Dao, T., et al. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning.</li></ol>',21))])}]]),l=JSON.parse('{"path":"/zh/posts/flashattention.html","title":"Flashattention","lang":"zh-CN","frontmatter":{"title":"Flashattention","date":"2025-05-10T00:00:00.000Z","category":["机器学习","深度学习","笔记"],"tag":["注意力机制","GPU优化","大模型","算法优化"],"cover":"/assets/images/cover3.jpg","isOriginal":true,"description":"简介 Flashattention是一种高效的注意力机制计算方法，由斯坦福大学研究团队在2022年提出。它通过优化GPU内存访问模式，显著提高了Transformer模型中注意力计算的速度并降低了内存使用。作为大模型训练和推理的关键优化技术，Flashattention已被广泛应用于各种大型语言模型中。","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Flashattention\\",\\"image\\":[\\"https://your-domain.com/Notes/Figure/FA1.png \\\\\\"FlashAttention Block Diagram\\\\\\"\\"],\\"datePublished\\":\\"2025-05-10T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-19T09:21:26.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/your-username\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/posts/flashattention.html"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"Flashattention"}],["meta",{"property":"og:description","content":"简介 Flashattention是一种高效的注意力机制计算方法，由斯坦福大学研究团队在2022年提出。它通过优化GPU内存访问模式，显著提高了Transformer模型中注意力计算的速度并降低了内存使用。作为大模型训练和推理的关键优化技术，Flashattention已被广泛应用于各种大型语言模型中。"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://your-domain.com/Notes/assets/images/cover3.jpg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-19T09:21:26.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://your-domain.com/Notes/assets/images/cover3.jpg"}],["meta",{"name":"twitter:image:alt","content":"Flashattention"}],["meta",{"property":"article:tag","content":"算法优化"}],["meta",{"property":"article:tag","content":"大模型"}],["meta",{"property":"article:tag","content":"GPU优化"}],["meta",{"property":"article:tag","content":"注意力机制"}],["meta",{"property":"article:published_time","content":"2025-05-10T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-19T09:21:26.000Z"}]]},"git":{"createdTime":1747123438000,"updatedTime":1747646486000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":8,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":2.2,"words":659},"filePathRelative":"zh/posts/flashattention.md","excerpt":"\\n<h2>简介</h2>\\n<p>Flashattention是一种高效的注意力机制计算方法，由斯坦福大学研究团队在2022年提出。它通过优化GPU内存访问模式，显著提高了Transformer模型中注意力计算的速度并降低了内存使用。作为大模型训练和推理的关键优化技术，Flashattention已被广泛应用于各种大型语言模型中。</p>\\n","autoDesc":true}')},6262:(i,a)=>{a.A=(i,a)=>{const t=i.__vccOpts||i;for(const[i,s]of a)t[i]=s;return t}}}]);