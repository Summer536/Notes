"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[4395],{2551:(e,a,t)=>{t.r(a),t.d(a,{comp:()=>m,data:()=>l});var o=t(641);const n=t.p+"assets/img/naive.bcddef04.png",s=t.p+"assets/img/safe.e36f31c5.png",r=t.p+"assets/img/online.b41e47c5.png",i={},m=(0,t(6262).A)(i,[["render",function(e,a){return(0,o.uX)(),(0,o.CE)("div",null,[a[0]||(a[0]=(0,o.Lk)("h1",{id:"softmax",tabindex:"-1"},[(0,o.Lk)("a",{class:"header-anchor",href:"#softmax"},[(0,o.Lk)("span",null,"Softmax")])],-1)),a[1]||(a[1]=(0,o.Lk)("h2",{id:"简介",tabindex:"-1"},[(0,o.Lk)("a",{class:"header-anchor",href:"#简介"},[(0,o.Lk)("span",null,"简介")])],-1)),a[2]||(a[2]=(0,o.Lk)("p",null,[(0,o.eW)("本文将讲解Softmax发展过程："),(0,o.Lk)("br"),(0,o.eW)(" naive softmax -> safe softmax -> online softmax")],-1)),(0,o.Q3)(" more "),a[3]||(a[3]=(0,o.Lk)("h2",{id:"naive-softmax",tabindex:"-1"},[(0,o.Lk)("a",{class:"header-anchor",href:"#naive-softmax"},[(0,o.Lk)("span",null,"Naive softmax")])],-1)),a[4]||(a[4]=(0,o.Lk)("p",null,[(0,o.eW)("原始softmax的公式为:"),(0,o.Lk)("br"),(0,o.eW)(" $$"),(0,o.Lk)("br"),(0,o.eW)(" \\sigma(z_i) = \\frac{e"),(0,o.Lk)("sup",{n:""},"{z_i}}{\\sum_{j=1}"),(0,o.eW)(" e^{z_j}}"),(0,o.Lk)("br"),(0,o.eW)(" $$")],-1)),a[5]||(a[5]=(0,o.Fv)('<ul><li><p>对于向量中的每一个元素，<strong>它的MAC(memory access count)为3</strong>: 第一次pass中load一次，在第二次pass中load一次, store一次, 所以一共是三次memory access。</p></li><li><p>Original softmax的问题所在: 在第三行的算法中，对进行sum的过程中，由于真实硬件的浮点格式所能表示的范围限制(fp16正数所能表示的最大值为65504，而$e^{12}$&gt;65504),很容易造成上溢或者下溢。<br> 为了解决上述问题，从而引出safe softmax。</p><figure><img src="'+n+'" alt="Naive softmax" tabindex="0" loading="lazy"><figcaption>Naive softmax</figcaption></figure></li></ul><h2 id="safe-softmax" tabindex="-1"><a class="header-anchor" href="#safe-softmax"><span>Safe softmax</span></a></h2>',2)),a[6]||(a[6]=(0,o.Lk)("p",null,[(0,o.eW)("为解决上述提到的可能的数据溢出问题，基本上所有的深度学习框架使用的都是safe Softmax的计算。其计算公式如下："),(0,o.Lk)("br"),(0,o.eW)(" $$"),(0,o.Lk)("br"),(0,o.eW)(" c = max(z_1,z_2,...,z_n)"),(0,o.Lk)("br"),(0,o.eW)(" $$"),(0,o.Lk)("br"),(0,o.eW)(" $$"),(0,o.Lk)("br"),(0,o.eW)(" \\sigma(z_i) = \\frac{e"),(0,o.Lk)("sup",{n:""},"{z_i-c}}{\\sum_{j=1}"),(0,o.eW)(" e^{z_j-c}}"),(0,o.Lk)("br"),(0,o.eW)(" $$")],-1)),a[7]||(a[7]=(0,o.Lk)("ul",null,[(0,o.Lk)("li",null,[(0,o.Lk)("p",null,[(0,o.eW)("该计算公式在数学上和naive等价:"),(0,o.Lk)("br"),(0,o.eW)(" $$"),(0,o.Lk)("br"),(0,o.eW)(" \\sigma(z_i) = \\frac{e"),(0,o.Lk)("sup",{n:""},"{z_i-c}}{\\sum_{j=1}"),(0,o.eW)(" e^{z_j-c}} = \\frac{e^{z_i} \\cdot e"),(0,o.Lk)("sup",{n:""},"{-c}}{\\sum_{j=1}"),(0,o.eW)(" e^{z_j} \\cdot e^{-c}} = \\frac{e"),(0,o.Lk)("sup",{n:""},"{z_i}}{\\sum_{j=1}"),(0,o.eW)(" e^{z_j}}"),(0,o.Lk)("br"),(0,o.eW)(" $$")])]),(0,o.Lk)("li",null,[(0,o.Lk)("p",null,[(0,o.eW)("Safe Softmax所带来的问题: 为了安全，我们需要额外求出输入向量中的元素最大值，这带来了多一次的循环pass，并且对于向量中的每一个元素，"),(0,o.Lk)("strong",null,"它的MAC(memory access count)为4"),(0,o.eW)("。具体表现为在第一次pass中Load $ z_i $ 一次, 在第二次pass中Load $ z_j $ 一次，在第三次pass中Load $ z_i $ 一次, Store $ \\sigma_{z_i} $ 一次, 所以总共mac是4次。"),(0,o.Lk)("br"),(0,o.eW)(" 为了解决上述问题，从而引出了Online Softmax。")]),(0,o.Lk)("figure",null,[(0,o.Lk)("img",{src:s,alt:"Safe softmax",tabindex:"0",loading:"lazy"}),(0,o.Lk)("figcaption",null,"Safe softmax")])])],-1)),a[8]||(a[8]=(0,o.Fv)('<h2 id="online-softmax" tabindex="-1"><a class="header-anchor" href="#online-softmax"><span>Online softmax</span></a></h2><p>在Safe的基础上，Online softmax做出的主要改进为: 将最大值 $c = max(z_1,z_2,...,z_n)$ 和归一因子 $d = \\sum_{j=1}^{n} e^{z_j-c}$ 放在同一个循环pass中处理。</p><h3 id="具体实现" tabindex="-1"><a class="header-anchor" href="#具体实现"><span>具体实现</span></a></h3><p>循环pass处理 $z_1$ -&gt; $z_n$：</p><ul><li><p>if $z_i \\leqslant c$ :<br> $$<br> c_{new} = c_{old}<br> $$<br> $$<br> d_{i} = d_{i-1} + e^{z_i-c_{new}}<br> $$</p></li><li><p>if $z_i &gt; c$ :<br> $$<br> c_{new} = z_i<br> $$<br> 之前计算的d是相较于旧的c即 $c_{old}$ 的，需要将其转换由新的 $c_{new}$计算:<br> $$<br> d_{old} = \\sum_{j=1}^{n} e^{z_j-c_{old}}, \\quad d_{new} = \\sum_{j=1}^{n} e^{z_j-c_{new}}<br> $$<br> 通过以下公式可进行转换:<br> $$<br> d_{new} = d_{old} \\cdot \\frac{e<sup>{c_{old}}}{e</sup>{c_{new}}}<br> $$<br> 因此对于归一化因子d更新时:<br> $$<br> d_{i} = d_{new} + e^{z_j-c_{new}} = d_{new} + e^{z_j-z_j} = d_{new} + 1<br> $$</p></li></ul><p><br><br><br> 该算法在迭代输入数组的元素时保留最大值c 和归一化项 d。在每次迭代中，它都需要将 normalizer d 调整为新的最大 cj，然后才向 normalizer 添加新的值。<br><strong>这里我们把vector中的每个元素的MAC从4降到了3</strong>，在第一次pass里面，我们load一次 $ z_j $ 即可，在第二次pass里面我们load一次 $ z_i $ ,store一次 $ \\sigma_{z_i} $,所以一共是3次memory access。</p><figure><img src="'+r+'" alt="Online softmax" tabindex="0" loading="lazy"><figcaption>online softmax</figcaption></figure><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h2><ol><li><p><a href="https://arxiv.org/pdf/1805.02867" target="_blank" rel="noopener noreferrer">Milakov M, Gimelshein N. Online normalizer calculation for softmax[J]. arXiv preprint arXiv:1805.02867, 2018</a></p></li><li><p><a href="https://zhuanlan.zhihu.com/p/1892986988065453222" target="_blank" rel="noopener noreferrer">从 Naive Softmax到Online Softmax and Top-k</a></p></li></ol>',9))])}]]),l=JSON.parse('{"path":"/zh/posts/softmax.html","title":"Softmax","lang":"zh-CN","frontmatter":{"title":"Softmax","date":"2025-05-20T00:00:00.000Z","readingTime":300,"category":["笔记"],"tag":["GPU优化","算法优化"],"cover":"/assets/images/cover3.jpg","isOriginal":true,"description":"简介 本文将讲解Softmax发展过程： naive softmax -> safe softmax -> online softmax","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Softmax\\",\\"image\\":[\\"https://your-domain.com/Notes/assets/images/cover3.jpg\\"],\\"datePublished\\":\\"2025-05-20T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-20T07:28:06.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/your-username\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/posts/softmax.html"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"Softmax"}],["meta",{"property":"og:description","content":"简介 本文将讲解Softmax发展过程： naive softmax -> safe softmax -> online softmax"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://your-domain.com/Notes/assets/images/cover3.jpg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-20T07:28:06.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://your-domain.com/Notes/assets/images/cover3.jpg"}],["meta",{"name":"twitter:image:alt","content":"Softmax"}],["meta",{"property":"article:tag","content":"算法优化"}],["meta",{"property":"article:tag","content":"GPU优化"}],["meta",{"property":"article:published_time","content":"2025-05-20T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-20T07:28:06.000Z"}]]},"git":{"createdTime":1747123438000,"updatedTime":1747726086000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":11,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":2.66,"words":799},"filePathRelative":"zh/posts/softmax.md","excerpt":"\\n<h2>简介</h2>\\n<p>本文将讲解Softmax发展过程：<br>\\nnaive softmax -&gt; safe softmax -&gt; online softmax</p>\\n","autoDesc":true}')},6262:(e,a)=>{a.A=(e,a)=>{const t=e.__vccOpts||e;for(const[e,o]of a)t[e]=o;return t}}}]);