"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[4395],{2954:(a,i,t)=>{t.r(i),t.d(i,{comp:()=>l,data:()=>r});var e=t(641),s=t(8341);const n={},l=(0,t(6262).A)(n,[["render",function(a,i){return(0,e.uX)(),(0,e.CE)("div",null,[i[0]||(i[0]=(0,e.Lk)("h1",{id:"softmax",tabindex:"-1"},[(0,e.Lk)("a",{class:"header-anchor",href:"#softmax"},[(0,e.Lk)("span",null,"Softmax")])],-1)),i[1]||(i[1]=(0,e.Lk)("h2",{id:"简介",tabindex:"-1"},[(0,e.Lk)("a",{class:"header-anchor",href:"#简介"},[(0,e.Lk)("span",null,"简介")])],-1)),i[2]||(i[2]=(0,e.Lk)("p",null,[(0,e.eW)("本文将讲解Softmax发展过程："),(0,e.Lk)("br"),(0,e.eW)(" naive softmax -> safe softmax -> online softmax")],-1)),(0,e.Q3)(" more "),i[3]||(i[3]=(0,e.Fv)('<h2 id="naive-softmax" tabindex="-1"><a class="header-anchor" href="#naive-softmax"><span>Naive softmax</span></a></h2><p>传统注意力机制的主要瓶颈：</p><ul><li>需要存储完整的注意力矩阵，导致内存使用呈二次方增长</li><li>大量的内存读写操作导致GPU计算效率低下</li></ul><p>Flashattention的创新点：</p><ul><li>使用分块计算策略（tiling）</li><li>充分利用GPU的SRAM（快速片上内存）</li><li>减少对HBM（高带宽内存）的访问</li></ul><h2 id="技术原理" tabindex="-1"><a class="header-anchor" href="#技术原理"><span>技术原理</span></a></h2><h3 id="算法流程" tabindex="-1"><a class="header-anchor" href="#算法流程"><span>算法流程</span></a></h3><figure><img src="'+s+'" alt="FlashAttention架构图" tabindex="0" loading="lazy"><figcaption>FlashAttention Block Diagram</figcaption></figure>',8)),(0,e.Q3)(' <p align="center">\n  <img src="Figure/FA1.png" width="500" alt="核心思想"/>\n</p> '),i[4]||(i[4]=(0,e.Fv)('<ol><li>将输入序列分成多个小块</li><li>每次只将一小块数据加载到SRAM中</li><li>在SRAM中计算局部注意力</li><li>根据数学等价性，合并局部结果得到全局注意力</li></ol><h3 id="性能提升" tabindex="-1"><a class="header-anchor" href="#性能提升"><span>性能提升</span></a></h3><ul><li>计算速度：比传统实现快2-4倍</li><li>内存使用：显著降低内存消耗，支持更长序列</li><li>训练效率：加速大模型训练过程</li></ul><h2 id="flashattention-2" tabindex="-1"><a class="header-anchor" href="#flashattention-2"><span>Flashattention-2</span></a></h2><p>在原始Flashattention基础上，研究团队进一步提出了Flashattention-2，带来了更多改进：</p><ul><li>优化了分块策略</li><li>改进了I/O复杂度</li><li>对不同GPU架构进行了专门优化</li></ul><h2 id="应用场景" tabindex="-1"><a class="header-anchor" href="#应用场景"><span>应用场景</span></a></h2><p>Flashattention已被广泛应用于：</p><ul><li>大型语言模型（如GPT系列）</li><li>长序列处理模型</li><li>视觉Transformer模型</li></ul><h2 id="实现和使用" tabindex="-1"><a class="header-anchor" href="#实现和使用"><span>实现和使用</span></a></h2><p>主要实现库：</p><ul><li>xFormers</li><li>Flash-Attention (GitHub)</li><li>PyTorch nightly版本已集成</li></ul><p>简单使用示例:</p><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#383A42;--shiki-dark:#abb2bf;--shiki-light-bg:#FAFAFA;--shiki-dark-bg:#282c34;"><pre class="shiki shiki-themes one-light one-dark-pro vp-code"><code><span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 使用FlashAttention-2的简化示例</span></span>\n<span class="line"><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">from</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> flash_attn </span><span style="--shiki-light:#A626A4;--shiki-dark:#C678DD;">import</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;"> flash_attn_func</span></span>\n<span class="line"></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># 假设q, k, v是查询、键、值矩阵</span></span>\n<span class="line"><span style="--shiki-light:#A0A1A7;--shiki-light-font-style:italic;--shiki-dark:#7F848E;--shiki-dark-font-style:italic;"># [batch_size, seq_len, num_heads, head_dim]</span></span>\n<span class="line"><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">output </span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#383A42;--shiki-dark:#61AFEF;"> flash_attn_func</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">(q, k, v, </span><span style="--shiki-light:#986801;--shiki-light-font-style:inherit;--shiki-dark:#E06C75;--shiki-dark-font-style:italic;">causal</span><span style="--shiki-light:#383A42;--shiki-dark:#56B6C2;">=</span><span style="--shiki-light:#986801;--shiki-dark:#D19A66;">True</span><span style="--shiki-light:#383A42;--shiki-dark:#ABB2BF;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="未来发展" tabindex="-1"><a class="header-anchor" href="#未来发展"><span>未来发展</span></a></h2><p>注意力机制优化仍在快速发展：</p><ul><li>支持更长的上下文窗口</li><li>降低显存需求</li><li>提高吞吐量和推理速度</li></ul><h2 id="参考文献" tabindex="-1"><a class="header-anchor" href="#参考文献"><span>参考文献</span></a></h2><ol><li><a href="https://arxiv.org/pdf/1805.02867" target="_blank" rel="noopener noreferrer">Milakov M, Gimelshein N. Online normalizer calculation for softmax[J]. arXiv preprint arXiv:1805.02867, 2018</a></li></ol>',19))])}]]),r=JSON.parse('{"path":"/zh/posts/softmax.html","title":"Softmax","lang":"zh-CN","frontmatter":{"title":"Softmax","date":"2025-05-20T00:00:00.000Z","readingTime":300,"category":["笔记"],"tag":["GPU优化","算法优化"],"cover":"/assets/images/cover3.jpg","isOriginal":true,"description":"简介 本文将讲解Softmax发展过程： naive softmax -> safe softmax -> online softmax","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"Softmax\\",\\"image\\":[\\"https://your-domain.com/Notes/assets/images/cover3.jpg\\"],\\"datePublished\\":\\"2025-05-20T00:00:00.000Z\\",\\"dateModified\\":\\"2025-05-20T05:54:27.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/your-username\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/posts/softmax.html"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"Softmax"}],["meta",{"property":"og:description","content":"简介 本文将讲解Softmax发展过程： naive softmax -> safe softmax -> online softmax"}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:image","content":"https://your-domain.com/Notes/assets/images/cover3.jpg"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-20T05:54:27.000Z"}],["meta",{"name":"twitter:card","content":"summary_large_image"}],["meta",{"name":"twitter:image:src","content":"https://your-domain.com/Notes/assets/images/cover3.jpg"}],["meta",{"name":"twitter:image:alt","content":"Softmax"}],["meta",{"property":"article:tag","content":"算法优化"}],["meta",{"property":"article:tag","content":"GPU优化"}],["meta",{"property":"article:published_time","content":"2025-05-20T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-20T05:54:27.000Z"}]]},"git":{"createdTime":1747123438000,"updatedTime":1747720467000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":10,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":1.65,"words":495},"filePathRelative":"zh/posts/softmax.md","excerpt":"\\n<h2>简介</h2>\\n<p>本文将讲解Softmax发展过程：<br>\\nnaive softmax -&gt; safe softmax -&gt; online softmax</p>\\n","autoDesc":true}')},6262:(a,i)=>{i.A=(a,i)=>{const t=a.__vccOpts||a;for(const[a,e]of i)t[a]=e;return t}},8341:(a,i,t)=>{a.exports=t.p+"assets/img/FAV1_1.25a3e8b3.png"}}]);