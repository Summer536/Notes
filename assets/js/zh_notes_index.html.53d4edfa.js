"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[2043],{6262:(t,e)=>{e.A=(t,e)=>{const a=t.__vccOpts||t;for(const[t,o]of e)a[t]=o;return a}},9731:(t,e,a)=>{a.r(e),a.d(e,{comp:()=>n,data:()=>i});var o=a(641);const r={},n=(0,a(6262).A)(r,[["render",function(t,e){return(0,o.uX)(),(0,o.CE)("div",null,e[0]||(e[0]=[(0,o.Fv)('<h1 id="学习笔记" tabindex="-1"><a class="header-anchor" href="#学习笔记"><span>学习笔记</span></a></h1><p>这里收集了我的技术学习笔记和心得体会，包括各种技术栈的学习记录。</p><h2 id="笔记列表" tabindex="-1"><a class="header-anchor" href="#笔记列表"><span>笔记列表</span></a></h2><ul><li><a href="https://summer536.github.io/Notes/zh/posts/pagedattention.html" target="_blank" rel="noopener noreferrer">PagedAttention</a> - 分页注意力机制计算方法的详细解析</li><li><a href="https://summer536.github.io/Notes/zh/posts/softmax.html" target="_blank" rel="noopener noreferrer">Naive -&gt; Safe -&gt; Online Softmax</a> - Softmax的发展</li><li><a href="https://summer536.github.io/Notes/zh/posts/flashattention.html" target="_blank" rel="noopener noreferrer">Flashattention技术详解</a> - 高效注意力机制计算方法的详细解析</li><li><a href="https://summer536.github.io/Notes/zh/posts/cuda-tech-stack.html" target="_blank" rel="noopener noreferrer">CUDA技术栈</a> - CUDA并行计算平台及编程模型介绍</li></ul>',4)]))}]]),i=JSON.parse('{"path":"/zh/notes/","title":"学习笔记","lang":"zh-CN","frontmatter":{"title":"学习笔记","index":false,"icon":"folder-open","category":["笔记"],"description":"学习笔记 这里收集了我的技术学习笔记和心得体会，包括各种技术栈的学习记录。 笔记列表 PagedAttention - 分页注意力机制计算方法的详细解析 Naive -> Safe -> Online Softmax - Softmax的发展 Flashattention技术详解 - 高效注意力机制计算方法的详细解析 CUDA技术栈 - CUDA并行计...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"学习笔记\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-05-21T13:05:48.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/Summer536\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/notes/"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"学习笔记"}],["meta",{"property":"og:description","content":"学习笔记 这里收集了我的技术学习笔记和心得体会，包括各种技术栈的学习记录。 笔记列表 PagedAttention - 分页注意力机制计算方法的详细解析 Naive -> Safe -> Online Softmax - Softmax的发展 Flashattention技术详解 - 高效注意力机制计算方法的详细解析 CUDA技术栈 - CUDA并行计..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-05-21T13:05:48.000Z"}],["meta",{"property":"article:modified_time","content":"2025-05-21T13:05:48.000Z"}]]},"git":{"createdTime":1747125782000,"updatedTime":1747832748000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":6,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":0.42,"words":125},"filePathRelative":"zh/notes/README.md","excerpt":"\\n<p>这里收集了我的技术学习笔记和心得体会，包括各种技术栈的学习记录。</p>\\n<h2>笔记列表</h2>\\n<ul>\\n<li><a href=\\"https://summer536.github.io/Notes/zh/posts/pagedattention.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">PagedAttention</a> - 分页注意力机制计算方法的详细解析</li>\\n<li><a href=\\"https://summer536.github.io/Notes/zh/posts/softmax.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Naive -&gt; Safe -&gt; Online Softmax</a> - Softmax的发展</li>\\n<li><a href=\\"https://summer536.github.io/Notes/zh/posts/flashattention.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">Flashattention技术详解</a> - 高效注意力机制计算方法的详细解析</li>\\n<li><a href=\\"https://summer536.github.io/Notes/zh/posts/cuda-tech-stack.html\\" target=\\"_blank\\" rel=\\"noopener noreferrer\\">CUDA技术栈</a> - CUDA并行计算平台及编程模型介绍</li>\\n</ul>","autoDesc":true}')}}]);