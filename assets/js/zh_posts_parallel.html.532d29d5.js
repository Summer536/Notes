"use strict";(self.webpackChunkvuepress=self.webpackChunkvuepress||[]).push([[9958],{6262:(a,s)=>{s.A=(a,s)=>{const n=a.__vccOpts||a;for(const[a,t]of s)n[a]=t;return n}},8079:(a,s,n)=>{n.r(s),n.d(s,{comp:()=>p,data:()=>i});var t=n(641);const e=n.p+"assets/img/all_reduce.f3803dfb.jpg",l=n.p+"assets/img/ringallreduce.cd20b5c5.jpg",m={},p=(0,n(6262).A)(m,[["render",function(a,s){return(0,t.uX)(),(0,t.CE)("div",null,[s[0]||(s[0]=(0,t.Lk)("h1",{id:"并行方法",tabindex:"-1"},[(0,t.Lk)("a",{class:"header-anchor",href:"#并行方法"},[(0,t.Lk)("span",null,"并行方法")])],-1)),s[1]||(s[1]=(0,t.Lk)("h2",{id:"简介",tabindex:"-1"},[(0,t.Lk)("a",{class:"header-anchor",href:"#简介"},[(0,t.Lk)("span",null,"简介")])],-1)),s[2]||(s[2]=(0,t.Lk)("p",null,[(0,t.eW)("本文将讲解四大并行方法："),(0,t.Lk)("br"),(0,t.eW)(" Data Parallel -> Model Parallel -> Pipeline Parallel -> Tensor Parallel")],-1)),s[3]||(s[3]=(0,t.Lk)("p",null,"给你8张卡、16张卡DP怎么做？TP怎么做？EP（专家并行）怎么做？甚至还要了解一下SP（序列并行）。",-1)),s[4]||(s[4]=(0,t.Lk)("p",null,"如 tensor_parallel_size、pipeline_parallel_size、enable_expert_parallel、data_parallel_size）来手动设置 TP、PP、EP、DP 等并行策略",-1)),(0,t.Q3)(" more "),s[5]||(s[5]=(0,t.Fv)('<h2 id="一、data-parallel" tabindex="-1"><a class="header-anchor" href="#一、data-parallel"><span>一、Data Parallel</span></a></h2><h2 id="二、model-parallel" tabindex="-1"><a class="header-anchor" href="#二、model-parallel"><span>二、Model Parallel</span></a></h2><h2 id="三、pipeline-parallel" tabindex="-1"><a class="header-anchor" href="#三、pipeline-parallel"><span>三、Pipeline Parallel</span></a></h2><h2 id="四、tensor-parallel" tabindex="-1"><a class="header-anchor" href="#四、tensor-parallel"><span>四、Tensor Parallel</span></a></h2><h2 id="附录-ring-allreduce" tabindex="-1"><a class="header-anchor" href="#附录-ring-allreduce"><span>附录：Ring-AllReduce</span></a></h2><p>AllReduce 的最终目标，就是让每块 GPU 上的数据都变成下图箭头右边汇总的结果。</p><figure><img src="'+e+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>Ring-AllReduce 是由百度提出的一种高效 <strong>All Reduce</strong> 算法，用于在分布式系统中进行数据同步。它通过环形拓扑结构，将数据分发到相邻的节点，从而实现高效的通信。<br> nvidia的NCCL通信库采用了这种算法。其通信流程如下图所示：</p><figure><img src="'+l+'" alt="" tabindex="0" loading="lazy"><figcaption></figcaption></figure><p>接下来计算的通信量只包括发送的参数量。假设有 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 个设备，模型参数总量为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi></mrow><annotation encoding="application/x-tex">\\Psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Ψ</span></span></span></span>，每个梯度块的大小为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\\Psi/N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Ψ/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>，每个设备只与其相邻的设备进行通信，首先讲解 Reduce-scatter 阶段：</p><ul><li><strong>步骤1</strong>：显卡 a 将 a0 发送给显卡 b，同时接受显卡 d 发送的 d3。</li><li><strong>步骤2</strong>：显卡 a 将 a3 + d3 发送给显卡 b，同时接受显卡 d 发送的 c2 + d2。</li><li><strong>步骤3</strong>：显卡 a 将 a2 + c2 + d2 发送给显卡 b，同时接受显卡 d 发送的 b1 + c1 + d1。</li></ul><p>Scatter-Reduce 阶段通信量：每次通信量是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\\Psi/N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Ψ/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>，一共进行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 次通信，总通信量为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\\Psi*(N-1)/N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Ψ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>。</p><p>接下来介绍 AllGather 阶段：</p><ul><li><strong>步骤1</strong>：显卡 a 将 a1 + b1 + c1 + d1 发送给显卡 b，显卡 b 直接做替换，同时接受显卡 d 发送的 a0 + b0 + c0 + d0，直接做替换。</li><li><strong>步骤2</strong>：显卡 a 将 a0 + b0 + c0 + d0 发送给显卡 b，显卡 b 直接做替换，同时接受显卡 d 发送的 a3 + b3 + c3 + d3，直接做替换。</li><li><strong>步骤3</strong>：显卡 a 将 a3 + b3 + c3 + d3 发送给显卡 b，显卡 b 直接做替换，同时接受显卡 d 发送的 a2 + b2 + c2 + d2，直接做替换。</li></ul><p>AllGather 阶段通信量：同样的，每次通信量是 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\\Psi/N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Ψ/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>，一共进行 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">N-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.7667em;vertical-align:-0.0833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6444em;"></span><span class="mord">1</span></span></span></span> 次通信，总通信量为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\\Psi*(N-1)/N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Ψ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>。</p><p>可以看到，<strong>单个设备通信量与 GPU 数量 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 无关</strong>，总通信量为：</p><p class="katex-block"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>N</mi><mo>+</mo><mi mathvariant="normal">Ψ</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>N</mi><mo>=</mo><mn>2</mn><mi mathvariant="normal">Ψ</mi><mo>∗</mo><mo stretchy="false">(</mo><mi>N</mi><mo>−</mo><mn>1</mn><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\\Psi*(N-1)/N + \\Psi*(N-1)/N = 2\\Psi*(N-1)/N </annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Ψ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">Ψ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2778em;"></span></span><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">2Ψ</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="mspace" style="margin-right:0.2222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span></span></p><p><strong>当GPU数量足够多时，总通信量趋近于 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mi mathvariant="normal">Ψ</mi></mrow><annotation encoding="application/x-tex">2\\Psi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord">2Ψ</span></span></span></span>，即总通信量与 GPU 数量无关。</strong></p><p>值得注意的是，使用张量并行加速时，分布式系统 Allreduce 的<strong>通信速度只受限于逻辑环中最慢的两个 GPU 的连接</strong>;（每次需要通信的数据大小仅为 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Ψ</mi><mi mathvariant="normal">/</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">\\Psi/N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Ψ/</span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span>，随着 <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.6833em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> 增大，通信量减少，一般小于 network bandwidth）；总结就是 Ring Allreduce 的通信速度恒定，和设备数量无关，完全由系统中GPU 之间最慢的连接决定。</p><h2 id="总结" tabindex="-1"><a class="header-anchor" href="#总结"><span>总结</span></a></h2><h2 id="待更新" tabindex="-1"><a class="header-anchor" href="#待更新"><span>待更新</span></a></h2><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2><p><a href="https://mp.weixin.qq.com/s/kYeNjMsesfKfoZtJPRkciA" target="_blank" rel="noopener noreferrer">大模型的分布式训练框架：deepspeed</a></p><p><a href="https://zhuanlan.zhihu.com/p/469942194" target="_blank" rel="noopener noreferrer">深度学习常见AllReduce算法图解</a></p>',24))])}]]),i=JSON.parse('{"path":"/zh/posts/parallel.html","title":"并行方法","lang":"zh-CN","frontmatter":{"title":"并行方法","date":"2025-06-25T00:00:00.000Z","readingTime":300,"category":["笔记"],"tag":["GPU优化","算法优化"],"isOriginal":true,"description":"简介 本文将讲解四大并行方法： Data Parallel -> Model Parallel -> Pipeline Parallel -> Tensor Parallel 给你8张卡、16张卡DP怎么做？TP怎么做？EP（专家并行）怎么做？甚至还要了解一下SP（序列并行）。 如 tensor_parallel_size、pipeline_paral...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"并行方法\\",\\"image\\":[\\"\\"],\\"datePublished\\":\\"2025-06-25T00:00:00.000Z\\",\\"dateModified\\":\\"2025-07-15T01:20:47.000Z\\",\\"author\\":[{\\"@type\\":\\"Person\\",\\"name\\":\\"GYQ\\",\\"url\\":\\"https://github.com/Summer536\\"}]}"],["meta",{"property":"og:url","content":"https://your-domain.com/Notes/zh/posts/parallel.html"}],["meta",{"property":"og:site_name","content":"GYQ的博客"}],["meta",{"property":"og:title","content":"并行方法"}],["meta",{"property":"og:description","content":"简介 本文将讲解四大并行方法： Data Parallel -> Model Parallel -> Pipeline Parallel -> Tensor Parallel 给你8张卡、16张卡DP怎么做？TP怎么做？EP（专家并行）怎么做？甚至还要了解一下SP（序列并行）。 如 tensor_parallel_size、pipeline_paral..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-07-15T01:20:47.000Z"}],["meta",{"property":"article:tag","content":"算法优化"}],["meta",{"property":"article:tag","content":"GPU优化"}],["meta",{"property":"article:published_time","content":"2025-06-25T00:00:00.000Z"}],["meta",{"property":"article:modified_time","content":"2025-07-15T01:20:47.000Z"}]]},"git":{"createdTime":1750855816000,"updatedTime":1752542447000,"contributors":[{"name":"yqgao","username":"yqgao","email":"gaoyuqing536@gmail.com","commits":3,"url":"https://github.com/yqgao"}]},"readingTime":{"minutes":2.68,"words":803},"filePathRelative":"zh/posts/parallel.md","excerpt":"\\n<h2>简介</h2>\\n<p>本文将讲解四大并行方法：<br>\\nData Parallel -&gt; Model Parallel -&gt; Pipeline Parallel -&gt; Tensor Parallel</p>\\n<p>给你8张卡、16张卡DP怎么做？TP怎么做？EP（专家并行）怎么做？甚至还要了解一下SP（序列并行）。</p>\\n<p>如 tensor_parallel_size、pipeline_parallel_size、enable_expert_parallel、data_parallel_size）来手动设置 TP、PP、EP、DP 等并行策略</p>\\n","autoDesc":true}')}}]);